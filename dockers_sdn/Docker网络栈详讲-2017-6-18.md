---
title: Docker网络栈详讲(home)
tags: Docker,Kubernetes,跨节点网络方案
grammar_cjkRuby: true
---
作者：{林战波}(Lynzabo)

全面讲解==Docker网络实现==，==Docker跨主机网络==，==Kubernetes网络==，==常用网络方案==。

[TOC]

# 1. Docker网络架构
## 1.1 Docker架构实现

Docker在1.9版本中引入了一整套的`docker network`子命令和跨主机网络支持。这允许用户可以根据他们应用的拓扑结构创建虚拟网络并将容器接入其所对应的网络。早在Docker1.7版本中，网络部分代码就已经被抽离并单独成为了Docker的网络库，即libnetwork。在此之后，容器的网络模式也被抽象变成了统一接口的驱动。
为了标准化网络驱动的开发步骤和支持多种网络驱动，Docker公司在libnetwork中使用了CNM（Container Network Model）。CNM定义了构建容器虚拟化网络的模型，同时还提供了可以用于开发多种网络驱动的标准化接口和组件。
libnetwork和Docker daemon及各个网络驱动的关系可以通过如图进行形象的表示。
![enter description here][1]

 Docker daemon通过调用libnetwork对外提供的API完成网络的创建和管理等功能。libnetwork中则使用了CNM来完成网络功能的提供。而CNM中主要有沙盒（sandbox）、端点（endpoint）和网络（network）这3种组件。libnetwork中内置的5种驱动则为libnetwork提供了不同类型的网络服务。下面分别对CNM中的3个核心组件和libnetwork中的5种内置驱动进行介绍。
CNM中的3个核心组件如下：
 - 沙盒：一个沙盒包含了一个容器网络栈的信息。沙盒可以对容器的接口、路由和DNS设置等进行管理。沙盒的实现可以使linux network namespace、FreeBSD Jail或者类似的机制。一个沙盒可以有多个端点和多个网络。
 - 端点：一个端点可以加入一个沙盒和一个网络。端点的实现可以是veth pair、Open vSwitch内部端口或者相似的设备。一个端点只可以属于一个网路并且只属于一个沙盒。
 - 网络：一个网络是一组可以直接互相联通的端点。网络的实现可以是Linux bridge、VLAN等。一个网络可以包含多个端点。
 
libnetwork中的5种内置驱动如下：
	 - bridge驱动  此驱动为Docker的默认设置，使用这个驱动的时候，libnetwork将创建出来的Docker容器连接到Docker网桥上，容器与外界通信使用NAT模式。
	 - host驱动  使用这种驱动的时候，libnetwork将不为Docker容器创建网络协议栈，即不会创建独立的network namespace。Docker容器中的进程处于宿主机的网络环境中，相当于Docker容器和宿主机公用同一个network namespace，使用宿主机的网卡、IP和端口等信息。但是，容器其他方面，如文件系统、进程列表等还是和宿主机隔离的。host模式很好地解决了容器与外界通信的地址转换问题，可以直接使用宿主机的IP进行通信，不存在虚拟化网络带来的额外性能负担。但是host驱动也降低了容器与容器之间、容器与宿主机之间网络层面的隔离性，引起网络资源的竞争与冲突，因此可以认为host驱动适用于对于容器集群规模不大的场景。
	 - overlay驱动  此驱动采用IETF标准的VXLAN方式，并且是VXLAN中被普遍认为最适合大规模的云计算虚拟化环境的SDN controller模式。在使用的过程中，需要一个额外的配置存储服务，例如Consul、etcd或Zk，还需要在启动Docker daemon的时候额外添加参数来指定所使用的配置存储服务地址。
	 - remote驱动  这个驱动实际上并未做真正的网络服务实现，而是调用了用户自行实现的网络驱动插件，使libnetwork实现了驱动的可插件化，更好地满足了用户的多种需求。用户只要根据libnetwork提供的协议标准，实现其所要求的各个接口并向Docker daemon进行注册。
	 - null驱动  使用这个驱动的时候，Docker容器拥有自己的network namespace，但是并不为Docker容器进行任何网络配置。也就是说，这个Docker容器除了network namespace自带的loopback网卡外，没有其他任何网卡、IP、路由等信息，需要用户为Docker容器添加网卡、配置IP等。这种模式如果不进行特定的配置是无法正常使用的，但是优点也非常明显，它给了用户最大的自由度来自定义容器的网络环境。

在引入libnetwork后，bridge、host、none不再是固定的网络模式了，后来docker才出来的支持网络叠加的overlay，而只是5种不同网络查件的实体。说他们是实体，因为现在用户可以利用Docker的网络命令创建更多与默认网络相似的网络，每一个都是特定类型网络插件的实体。
*关于这个新的网络插件形式，在1.5中【新的Link介绍】详细讲，会详细讲不通network的容器是不能互通的。*

下面是演示libnetwork的例子：
![enter description here][2]
![enter description here][3]
![enter description here][4]
使用下面命令创建名为backend和frontend的两个网络（网桥）
![enter description here][5]
![enter description here][6]
```shell?linenums
[root@localhost ~]# docker exec container1 ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
10: eth0@if11: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.18.0.2/16 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe12:2/64 scope link
       valid_lft forever preferred_lft forever
[root@localhost ~]# docker exec container2 ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
12: eth0@if13: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:42:ac:12:00:03 brd ff:ff:ff:ff:ff:ff
    inet 172.18.0.3/16 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe12:3/64 scope link
       valid_lft forever preferred_lft forever
[root@localhost ~]# docker exec container2 ping 172.18.0.2
PING 172.18.0.2 (172.18.0.2): 56 data bytes
64 bytes from 172.18.0.2: seq=0 ttl=64 time=0.058 ms
64 bytes from 172.18.0.2: seq=1 ttl=64 time=0.106 ms
^C
[root@localhost ~]# docker exec container3 ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
14: eth0@if15: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:42:ac:13:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.19.0.2/16 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe13:2/64 scope link
       valid_lft forever preferred_lft forever
[root@localhost ~]# docker exec container2 ping 172.19.0.2
^C
[root@localhost ~]#
```

可以在container2中使用命令ifconfig来查看此容器中的网卡及其配置情况。可以看到，此容器中只有一块以太网卡，其名称为eth0，并且配置了和网桥backend同在一个IP段的IP地址，这个网卡就是CNM模型中的端点。
最后，使用如下命令将container2加入到frontend网络中。
命令如下：
```shell?linenums
docker network connect frontend container2
```
详细操作如下：
```shell?linenums
[root@localhost ~]# docker exec container2 ifconfig
eth0      Link encap:Ethernet  HWaddr 02:42:AC:12:00:03
          inet addr:172.18.0.3  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:acff:fe12:3/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:286 errors:0 dropped:0 overruns:0 frame:0
          TX packets:516 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:26380 (25.7 KiB)  TX bytes:48976 (47.8 KiB)
lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
[root@localhost ~]# docker network connect frontend container2
[root@localhost ~]# docker exec container2 ifconfig
eth0      Link encap:Ethernet  HWaddr 02:42:AC:12:00:03
          inet addr:172.18.0.3  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:acff:fe12:3/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:317 errors:0 dropped:0 overruns:0 frame:0
          TX packets:572 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:29250 (28.5 KiB)  TX bytes:54296 (53.0 KiB)
eth1      Link encap:Ethernet  HWaddr 02:42:AC:13:00:03
          inet addr:172.19.0.3  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:acff:fe13:3/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:9 errors:0 dropped:0 overruns:0 frame:0
          TX packets:9 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:746 (746.0 B)  TX bytes:746 (746.0 B)
lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
[root@localhost ~]#
```
再次，在container2中使用命令ifconfig来查看此容器中的网卡及其配置情况。发现多了一块名为eth1的以太网卡，并且其IP和网桥frontend同在一个IP段。测试container2与container3的连通性后，可以发现两者已经互通。
```shell?linenums
root@localhost ~]# docker exec container2 ping 172.19.0.2
PING 172.19.0.2 (172.19.0.2): 56 data bytes
64 bytes from 172.19.0.2: seq=0 ttl=64 time=0.063 ms
64 bytes from 172.19.0.2: seq=1 ttl=64 time=0.069 ms
^C
[root@localhost ~]#
```
可以看出，`docker network connect`命令会在所连接的容器中创建新的网卡，以完成其与所指定网络的连接。

## 1.2 bridge桥接驱动实现机制分析

此驱动为Docker的默认设置，使用这个驱动的时候，libnetwork将创建出来的Docker容器连接到Docker网桥上，容器与外界通信使用NAT模式。

### docker0网桥
![enter description here][7]
![enter description here][8]
![enter description here][9]
网桥就等同于交换机，为连在其上的设备转发数据帧。网桥上的veth网卡设备相当于交换机上的端口，可以将多个容器或虚拟机连接在其上，这些端口工作在二层，所以是不需要配置IP信息的。途中docker0网桥就为其上的容器转发数据帧，使得同一台宿主机上的Docker容器之间可以相互通信。docker是普通的linux网桥，是二层设备也能配置IP，可以认为其内部有一个可以用于配置IP信息的网卡接口。在Docker的桥接网络模式中，docker0的IP地址作为连于之上的容器的默认网关地址存在。
![enter description here][10]

		* Linux Bridge、VETH和Network Namespace
		Linux Bridge ，即Linux网桥设备，是Linux提供的一种虚拟网络设备之一。其工作方式非常类似于物理的网络交换机设备。Linux Bridge可以工作在二层，也可以工作在三层，默认工作在二层。工作在二层时，可以在同一网络的不同主机间转发以太网报文；一旦你给一个Linux Bridge分配了IP地址，也就开启了该Bridge的三层工作模式。在Linux下，你可以用 iproute2 工具包或brctl命令对Linux bridge进行管理。
		VETH(Virtual Ethernet )是Linux提供的另外一种特殊的网络设备，中文称为虚拟网卡接口。它总是成对出现，要创建就创建一个pair。一个Pair中的veth就像一个网络线缆的两个端点，数据从一个端点进入，必然从另外一个端点流出。每个veth都可以被赋予IP地址，并参与三层网络路由过程。
		关于Linux Bridge和VETH的具体工作原理，可以参考IBM developerWorks上的这篇文章《 Linux 上的基础网络设备详解 》。
		Network namespace，网络名字空间，允许你在Linux创建相互隔离的网络视图，每个网络名字空间都有独立的网络配置，比如：网络设备、路由表等。新建的网络名字空间与主机默认网络名字空间之间是隔离的。我们平时默认操作的是主机的默认网络名字空间。
		概念总是抽象的，接下来我们将在一个模拟Docker容器网络的例子中看到这些Linux网络概念和网络设备到底是起到什么作用的以及是如何操作的。
#### 1. 模拟一个拥有两个容器的容器桥接网络
**我们来模拟一个拥有两个容器的容器桥接网络**：
![enter description here][11]
对应的用手工搭建的模拟版本拓扑如下(由于在同一台主机，模拟版本采用172.16.0.0/16网段)：
![enter description here][12]

**不需要安装Docker**
创建步骤：
a) 创建Container_ns1和Container_ns2 network namespace

默认情况下，我们在Host上看到的都是default network namespace的视图。为了模拟容器网络，我们新建两个network namespace：
```shell?linenums
[root@localhost ~]# ip netns add Container_ns1
[root@localhost ~]# ip netns add Container_ns2
[root@localhost ~]# ip netns list
Container_ns2
Container_ns1
[root@localhost ~]#
```
创建的ns也可以在/var/run/netns路径下看到：
```shell?linenums
[root@localhost ~]# ls /var/run/netns -l
total 0
-r--r--r--. 1 root root 0 Jun 14 08:01 Container_ns1
-r--r--r--. 1 root root 0 Jun 14 08:01 Container_ns2
[root@localhost ~]#
```
我们探索一下新创建的ns的网络空间(通过`ip netns exec`命令可以在特定ns的内部执行相关程序，这个exec命令是至关重要的，后续还会发挥更大作用)：
```shell?linenums
[root@localhost ~]# ip netns exec Container_ns1 ip addr
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
[root@localhost ~]#
[root@localhost ~]# ip netns exec Container_ns2 ip addr
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
[root@localhost ~]#
```
可以看到，新建的ns的网络设备只有一个loopback口，并且路由表为空。
b) 创建MyDocker0 bridge
在Linux下，可以使用brctl命令查看和管理网桥（需要安装bridge-utils软件包），查看本机上的Linux网桥以及其上的端口：
```shell?linenums
[root@localhost ~]# brctl show
bridge name	bridge id		STP enabled	interfaces
[root@localhost ~]#
```
我们在default network namespace下创建MyDocker0 linux bridge：
```shell?linenums
[root@localhost ~]# brctl addbr MyDocker0
[root@localhost ~]# brctl show
bridge name	bridge id		STP enabled	interfaces
MyDocker0		8000.000000000000	no
[root@localhost ~]#
```
给MyDocker0分配ip地址并生效该设备，开启三层，为后续充当Gateway做准备：
```shell?linenums
[root@localhost ~]# ip addr add 172.16.1.254/16 dev MyDocker0
[root@localhost ~]# ip link set dev MyDocker0 up
[root@localhost ~]#
```
启用后，我们发现default network namespace的路由配置中增加了一条路由,172.16.0.0：
```shell?linenums
[root@localhost ~]# routel
         target            gateway          source    proto    scope    dev tbl
      10.0.2.0/ 24                       10.0.2.15   kernel     link   eth0
    172.16.0.0/ 16                    172.16.1.254   kernel     linkMyDocker0
       10.0.2.0          broadcast       10.0.2.15   kernel     link   eth0 local
      10.0.2.15              local       10.0.2.15   kernel     host   eth0 local
     10.0.2.255          broadcast       10.0.2.15   kernel     link   eth0 local
      127.0.0.0          broadcast       127.0.0.1   kernel     link     lo local
     127.0.0.0/ 8            local       127.0.0.1   kernel     host     lo local
      127.0.0.1              local       127.0.0.1   kernel     host     lo local
127.255.255.255          broadcast       127.0.0.1   kernel     link     lo local
     172.16.0.0          broadcast    172.16.1.254   kernel     linkMyDocker0 local
   172.16.1.254              local    172.16.1.254   kernel     hostMyDocker0 local
 172.16.255.255          broadcast    172.16.1.254   kernel     linkMyDocker0 local
            ::/ 96     unreachable                                       lo
::ffff:0.0.0.0/ 96     unreachable                                       lo
    2002:a00::/ 24     unreachable                                       lo
   2002:7f00::/ 24     unreachable                                       lo
   2002:a9fe::/ 32     unreachable                                       lo
   2002:ac10::/ 28     unreachable                                       lo
   2002:c0a8::/ 32     unreachable                                       lo
   2002:e000::/ 19     unreachable                                       lo
   3ffe:ffff::/ 32     unreachable                                       lo
        fe80::/ 64                                   kernel            eth0
        fe80::/ 64                                   kernel         MyDocker0
        default        unreachable                   kernel              lo unspec
            ::1              local                     none              lo local
fe80::445b:38ff:fe1b:ea5a              local                     none              lo local
fe80::5054:ff:fe88:15b6              local                     none              lo local
        ff00::/ 8                                                      eth0 local
        ff00::/ 8                                                   MyDocker0 local
        default        unreachable                   kernel              lo unspec
[root@localhost ~]#
```

c) 创建VETH，连接两对network namespaces
到目前为止，default ns与Container_ns1、Container_ns2之间还没有任何瓜葛。接下来就是见证奇迹的时刻了。我们通过veth pair建立起多个ns之间的联系：
创建连接default ns与Container_ns1之间的veth pair – `veth1`和`veth1p`：
```shell?linenums
[root@localhost ~]# ip link add veth1 type veth peer name veth1p
[root@localhost ~]# ip -d link show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 promiscuity 0 addrgenmode eui64
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 52:54:00:88:15:b6 brd ff:ff:ff:ff:ff:ff promiscuity 0 addrgenmode eui64
3: MyDocker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT qlen 1000
    link/ether 46:5b:38:1b:ea:5a brd ff:ff:ff:ff:ff:ff promiscuity 0
    bridge forward_delay 1500 hello_time 200 max_age 2000 addrgenmode eui64
4: veth1p@veth1: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 1000
    link/ether 62:47:01:68:5b:fd brd ff:ff:ff:ff:ff:ff promiscuity 0
    veth addrgenmode eui64
5: veth1@veth1p: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 1000
    link/ether 22:68:46:ec:47:8f brd ff:ff:ff:ff:ff:ff promiscuity 0
    veth addrgenmode eui64
[root@localhost ~]#
```
我们看到`veth1`和`veth1p`了。

将veth1设备对端点--{“插到”}(insert)--MyDocker0这个bridge上，我们看到interfaces里包含了veth1设备对端点：
```shell?linenums
[root@localhost ~]# brctl addif MyDocker0 veth1
[root@localhost ~]# ip link set veth1 up
[root@localhost ~]# brctl show
bridge name	bridge id		STP enabled	interfaces
MyDocker0		8000.226846ec478f	no		veth1
[root@localhost ~]#
```
将veth1p设备对端点--{“放入”}(add)--到Container_ns1 ns中：
```shell?linenums
[root@localhost ~]# ip link set veth1p netns Container_ns1
[root@localhost ~]# ip netns exec Container_ns1 ip addr
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
4: veth1p@if5: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 62:47:01:68:5b:fd brd ff:ff:ff:ff:ff:ff link-netnsid 0
[root@localhost ~]#
```
这时，你在default ns中将看不到veth1p这个虚拟网络设备了。
```shell?linenums
[root@localhost ~]# ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 52:54:00:88:15:b6 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 84912sec preferred_lft 84912sec
    inet6 fe80::5054:ff:fe88:15b6/64 scope link
       valid_lft forever preferred_lft forever
3: MyDocker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN qlen 1000
    link/ether 22:68:46:ec:47:8f brd ff:ff:ff:ff:ff:ff
    inet 172.16.1.254/16 scope global MyDocker0
       valid_lft forever preferred_lft forever
    inet6 fe80::445b:38ff:fe1b:ea5a/64 scope link
       valid_lft forever preferred_lft forever
5: veth1@if4: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue master MyDocker0 state LOWERLAYERDOWN qlen 1000
    link/ether 22:68:46:ec:47:8f brd ff:ff:ff:ff:ff:ff link-netnsid 0
[root@localhost ~]#
```
按照上面拓扑，位于Container_ns1中的veth应该更名为eth0：
```shell?linenums
[root@localhost ~]# ip netns exec Container_ns1 ip addr
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
4: veth1p@if5: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 62:47:01:68:5b:fd brd ff:ff:ff:ff:ff:ff link-netnsid 0
[root@localhost ~]# 
[root@localhost ~]# ip netns exec Container_ns1 ip link set veth1p name eth0
[root@localhost ~]# 
[root@localhost ~]# ip netns exec Container_ns1 ip addr
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
4: eth0@if5: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 62:47:01:68:5b:fd brd ff:ff:ff:ff:ff:ff link-netnsid 0
[root@localhost ~]#
```
将Container_ns1中的eth0生效
```shell?linenums
[root@localhost ~]# ip netns exec Container_ns1 ip link set eth0 up
[root@localhost ~]#
```
将Container_ns1中的eth0配置IP地址：
```shell?linenums
[root@localhost ~]#
[root@localhost ~]# ip netns exec Container_ns1 ip addr add 172.16.1.1/16 dev eth0
[root@localhost ~]#
```
赋予IP地址后，自动生成一条直连路由：
```shell?linenums
[root@localhost ~]# ip netns exec Container_ns1 ip route
172.16.0.0/16 dev eth0  proto kernel  scope link  src 172.16.1.1
[root@localhost ~]#
```
现在在Container_ns1下可以ping通MyDocker0了，但由于没有其他路由，包括默认路由，ping其他地址还是不通的（比如：docker0的地址：172.17.0.1）：
```shell?linenums
[root@localhost ~]# ip netns exec Container_ns1 ip route
172.16.0.0/16 dev eth0  proto kernel  scope link  src 172.16.1.1
[root@localhost ~]# ip netns exec Container_ns1 ping -c 3 172.16.1.254
PING 172.16.1.254 (172.16.1.254) 56(84) bytes of data.
64 bytes from 172.16.1.254: icmp_seq=1 ttl=64 time=0.035 ms
64 bytes from 172.16.1.254: icmp_seq=2 ttl=64 time=0.030 ms
64 bytes from 172.16.1.254: icmp_seq=3 ttl=64 time=0.032 ms

--- 172.16.1.254 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2005ms
rtt min/avg/max/mdev = 0.030/0.032/0.035/0.005 ms
[root@localhost ~]# ip netns exec Container_ns1 ping -c 3 172.17.0.1
connect: Network is unreachable
[root@localhost ~]#
```
我们再给Container_ns1添加一条默认路由，让其能ping通物理主机上的其他网络设备或其他ns空间中的网络设备地址：
```shell?linenums
[root@localhost ~]# ip netns exec Container_ns1 ip route add default via 172.16.1.1
[root@localhost ~]# ip netns exec Container_ns1 ip route
default via 172.16.1.1 dev eth0
172.16.0.0/16 dev eth0  proto kernel  scope link  src 172.16.1.1
[root@localhost ~]#
```

```shell?linenums
[root@localhost ~]# ip netns exec Container_ns1 ping -c 3 172.17.0.1
PING 172.17.0.1 (172.17.0.1) 56(84) bytes of data.
64 bytes from 172.17.0.1: icmp_seq=1 ttl=64 time=0.068 ms
64 bytes from 172.17.0.1: icmp_seq=2 ttl=64 time=0.076 ms
64 bytes from 172.17.0.1: icmp_seq=3 ttl=64 time=0.069 ms

--- 172.17.0.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 1999ms
rtt min/avg/max/mdev = 0.068/0.071/0.076/0.003 ms
[root@localhost ~]#
```
不过这时候，如果想在Container_ns1中ping通物理主机之外的地址，比如:google.com，那还是不通的。为什么呢？因为ping的icmp的包的源地址没有做snat（docker是通过设置 iptables 规则实现的），导致出去的以172.16.1.1为源地址的包“有去无回”了^0^。（后面补充这个）
//TODO

接下来，我们按照上述步骤，再创建连接default ns与Container_ns2之间的veth pair – `veth2`和`veth2p`：
```shell?linenums
[root@localhost ~]# 创建veth设备对veth2和veth2p
[root@localhost ~]# ip link add veth2 type veth peer name veth2p
[root@localhost ~]# 将veth2设备对端点插入MyDocker0网桥
[root@localhost ~]# brctl addif MyDocker0 veth2
[root@localhost ~]# #启动veth2设备对
[root@localhost ~]# ip link set veth2 up
[root@localhost ~]#
[root@localhost ~]# 将veth2p设备对放入Container_ns2
[root@localhost ~]# ip link set veth2p netns Container_ns2
[root@localhost ~]# 重命名veth2p设备对名称为eth0
[root@localhost ~]# ip netns exec Container_ns2 ip link set veth2p name eth0
[root@localhost ~]# 启动Container_ns2 ns中的eth0
[root@localhost ~]# ip netns exec Container_ns2 ip link set eth0 up
[root@localhost ~]# 设置Container_ns2 ns中的eth0的IP
[root@localhost ~]# ip netns exec Container_ns2 ip addr add 172.16.1.2/16 dev eth0
[root@localhost ~]# 设置能ping通物理主机上的其他网络设备或其他ns空间中的网络设备地址
[root@localhost ~]# ip netns exec Container_ns2 ip route add default via 172.16.1.2
```
当然此时两个ns之间连通，主要还是通过直连网络，实质上是MyDocker0在二层起到的作用。以在Container_ns1中ping Container_ns2的eth0地址为例。
Container_ns1此时的路由表：
```shell?linenums
[root@localhost ~]# ip netns exec Container_ns1 ip route
default via 172.16.1.1 dev eth0
172.16.0.0/16 dev eth0  proto kernel  scope link  src 172.16.1.1
[root@localhost ~]#
```
**ping 172.16.1.2执行后，根据路由表，将首先匹配到直连网络（第二条），即无需gateway转发便可以直接将数据包送达。arp查询后（要么从arp cache中找到，要么在MyDocker0这个二层交换机中泛洪查询）获得172.16.1.2的mac地址。ip包的目的ip填写172.16.1.2，二层数据帧封包将目的mac填写为刚刚查到的mac地址，通过数据链路层发送出去。这一过程就是一个标准的二层交换机的数据报文交换过程。
而如果是在Container_ns1中ping 172.17.0.1(同主机上的docker0 bridge的地址)，那么MyDocker0将在三层起到作用。即ping 172.17.0.1执行后，根据路由表，没有匹配到直连网络，只能通过default路由将数据包通过发到eth0发给172.16.1.1。虽然都是发给172.16.1.1，但这次更类似于“数据被直接发到 Bridge 上，而不是Bridge从一个端口接收(这块儿与我之前的文章中的理解稍有差异)”。二层的目的mac地址填写的是gateway 172.16.1.1自己的mac地址（Bridge的mac地址），此时的MyDocker0更像是一块普通网卡的角色。MyDocker0收到数据包后，发现并非是发给自己的ip包，通过主机路由表找到直连链路路由，MyDocker0将数据包Forward到docker0上（封装的二层数据包的目的MAC地址为docker0的mac地址）。此时的docker0也是一种“网卡”的角色。

#### 2. 使用ip命令配置并查看Docker容器网络namespace
Docker正是使用Linux namespace技术进行资源隔离的，网络也是如此。当用默认网络模式（bridge模式）启动一个Docker容器时，一定是在主机上新创建了一个Linux network namespace。用户可以按照在network namespace中配置网络的方法来配置Docker容器的网络。
首先，启动一个名为test1的Docker容器：
```shell?linenums
docker run -itd --name test1 ubuntu /bin/bash
```
然后，使用`ip netns list`命令查看是否可以看到新创建的network namespace。执行命令后发现没有看到新增加的network namespce。这并不代表Docker容器没有创建network namespace，只是`ip netns`命令无法查看罢了，这与`ip netns`命令的工作方式有关。
当使用`ip netns`命令创建了两个network namespace（ns1和ns2）后，会在/var/run/netns目录下看到ns1和ns2两项：
![enter description here][13]
`ip netns list`命令在/var/run/netns目录下查找network namespace。由于Docker创建network namespace并不在此目录下创建任何项，因此，需要一些额外的操作来使ip命令可以操纵Docker创建的network namespace。
Linux下的每一个进程都都会属于一个特定的network namespace，来看一下不同network namespace环境中/proc/$PID/ns目录下有何区别。
![enter description here][14]
![enter description here][15]

当docker启动一个容器时，首先会创建一对虚拟以太网设备（VETH, Virtual Ethernet Device).这个veth设备总是成对创建的，把它们想象成是用一根网线连接起来的两个网口就好了。创建veth需要指定两个口的名字，docker会随机创建它们。然后docker把其中一个端口关联到docker0网桥上，相当于下面的命令：
```shell?linenums
brctl  addif docker0 veth-xxxA
```
想象成是把网线插入到交换机的一个网口中就行了。

那么另一端veth-xxxB呢？不要发挥想象力，认为要插入一台主机。在一定程度上，docker容器可以被看作是虚拟机，只是这种虚拟是通过namespace的隔离来实现的。所以本质上容器内的进程和宿主机上的进程没什么不同，只是各自关联的namespace不同而已。当然，namespace上施加的限制也不同。关于namespace，可以参考 namespaces - overview of Linux PID namespaces 。为了把veth-xxxB个容器关联起来，docker会把veth-xxxB移到容器所在的network namespace中，并改名成eth0.于是，在容器中就能看到eth0设备了，但实际上和宿主机的eth0并没有什么关系。接下来，docker会给veth-xxxB分配ip地址--当然还包括路由设置，ip是从配置好的地址池中选择的。更详细的关于地址分配的工作流程，我还没有在文档中看到。

一般来说，分属不同网络空间的设备是不能直接通信的，但veth设备设备对是个例外，虽然分属不同的网络空间了，但是Linux内核允许两者通信。这样，从容器中经eth0发出的数据包会被传递到veth-xxxA, veth-xxxA再传递给docker0.但是docker0和物理网卡eth0之间并没有连接起来，因此，以太数据包到此就结束了，没法传到物理网络上去的。如果我们希望docker0是一个和外部隔绝的网络，这恰好就是想要的。可是，如果要和外部网络通信怎么办？这时候就需要路由器的帮助了。docker0和eth0此时都是在宿主机的名字空间可见的，通过iptables设置nat实现docker0到eth0的通信。

#### 3. Docker0的“双重身份”
我们来理解一下Docker0这个软网桥：
![enter description here][16]
1、从容器视角，网桥（交换机）身份

docker0对于通过veth pair“插在”网桥上的container1和container2来说，首先就是一个二层的交换机的角色：泛洪、维护cam表，在二层转发数据包；同 时由于docker0自身也具有mac地址（这个与纯二层交换机不同），并且绑定了ip(这里是172.17.0.1)，因此在 container中还作为container default路由的默认Gateway而存在。

2、从宿主机视角，网卡身份

物理交换机提供了由硬件实现的高效的背板通道，供连接在交换机上的主机高效实现二层通信；对于开启了三层协议的物理交换机而言，其ip路由的处理 也是由物理交换机管理程序提供的。对于docker0而言，其负责处理二层交换机逻辑以及三层的处理程序其实就是宿主机上的Linux内核 tcp/ip协议栈程序。而从宿主机来看，所有docker0从veth（只是个二层的存在，没有绑定ipv4地址）接收到的数据包都会被宿主机 看成从docker0这块网卡（第二个身份，绑定172.17.0.1)接收进来的数据包，尤其是在进入三层时，宿主机上的iptables就会 对docker0进来的数据包按照rules进行相应处理（通过一些内核网络设置也可以忽略docker0 brigde数据的处理）。

在后续的Docker容器网络通信流程分析中，docker0将在这两种身份间来回切换。

### iptables规则

端口映射让位于容器中的service可以将服务范围扩展到主机之外，比如：一个运行于container中的nginx可以通过宿主机的9091端口对外提供http server服务：
```shell?linenums
[root@localhost ~]# docker run -d -p 9091:80 nginx:latest
Unable to find image 'nginx:latest' locally
Trying to pull repository docker.io/library/nginx ...
latest: Pulling from docker.io/library/nginx
ff3d52d8f55f: Pull complete
226f4ec56ba3: Pull complete
53d7dd52b97d: Pull complete
Digest: sha256:41ad9967ea448d7c2b203c699b429abe1ed5af331cd92533900c6d77490e0268
765b4ace1971721391fabdf34a85803e8c8af9b9fe0b0769ec03696c2773d72a
[root@localhost ~]# ifconfig
-bash: ifconfig: command not found
[root@localhost ~]# ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 52:54:00:88:15:b6 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86149sec preferred_lft 86149sec
    inet6 fe80::5054:ff:fe88:15b6/64 scope link
       valid_lft forever preferred_lft forever
3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP
    link/ether 02:42:fe:00:45:90 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 scope global docker0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:feff:fe00:4590/64 scope link
       valid_lft forever preferred_lft forever
5: veth2425253@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP
    link/ether 3e:17:db:8e:14:db brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet6 fe80::3c17:dbff:fe8e:14db/64 scope link
       valid_lft forever preferred_lft forever
[root@localhost ~]# 
[root@localhost ~]# curl http://10.0.2.15:9091
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
[root@localhost ~]#
```
Docker针对端口映射前后有两种方案，一种是1.7版本之前docker-proxy+iptables DNAT 的方式；另一种则是1.7版本(及之后)提供的完全由iptables DNAT实现的端口映射。不过在目前docker 1.9.1中，前一种方式依旧是默认方式。但是从Docker 1.7版本起，Docker提供了一个配置项：–userland-proxy，采用userland proxy(–userland-proxy=true)为每个expose端口的容器启动一个proxy实例来做端口流量转发，以让Docker用户决定是否启用docker-proxy，默认为true，即启用docker-proxy。
```shell?linenums
$ ps -ef|grep docker-proxy
root     26246  6228  0 16:18 ?        00:00:00 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 9091 -container-ip 172.17.0.2 -container-port 80
```
docker-proxy实际上就是在default ns和container ns之间转发流量而已。我们完全可以模拟这一过程。
我们创建一个fileserver demo：
```go?linenums
//testfileserver.go
package main

import "net/http"

func main() {
    http.ListenAndServe(":8080", http.FileServer(http.Dir(".")))
}
```
我们在Container_ns1下启动这个Fileserver service：
```shell?linenums
$ sudo ip netns exec Container_ns1 ./testfileserver
$
$ sudo ip netns exec Container_ns1 lsof -i tcp:8080
COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
testfiles 3605 root    3u  IPv4 297022      0t0  TCP *:http-alt (LISTEN)
```
可以看到在Container_ns1下面，8080已经被testfileserver监听，不过在default ns下，8080端口依旧是avaiable的。

接下来，我们在default ns下创建一个简易的proxy：
```go?linenums
//proxy.go
... ...

var (
    host          string
    port          string
    container     string
    containerport string
)

func main() {
    flag.StringVar(&host, "host", "0.0.0.0", "host addr")
    flag.StringVar(&port, "port", "", "host port")
    flag.StringVar(&container, "container", "", "container addr")
    flag.StringVar(&containerport, "containerport", "8080", "container port")

    flag.Parse()

    fmt.Printf("%s\n%s\n%s\n%s", host, port, container, containerport)

    ln, err := net.Listen("tcp", host+":"+port)
    if err != nil {
        // handle error
        log.Println("listen error:", err)
        return
    }
    log.Println("listen ok")

    for {
        conn, err := ln.Accept()
        if err != nil {
            // handle error
            log.Println("accept error:", err)
            continue
        }
        log.Println("accept conn", conn)
        go handleConnection(conn)
    }
}

func handleConnection(conn net.Conn) {
    cli, err := net.Dial("tcp", container+":"+containerport)
    if err != nil {
        log.Println("dial error:", err)
        return
    }
    log.Println("dial ", container+":"+containerport, " ok")

    go io.Copy(conn, cli)
    _, err = io.Copy(cli, conn)
    fmt.Println("communication over: error:", err)
}
```
在default ns下执行：
```shell?linenums
./proxy -host 0.0.0.0 -port 9090 -container 172.16.1.1 -containerport 8080
0.0.0.0
9090
172.16.1.1
80802017/01/11 17:26:10 listen ok
```
我们http get一下宿主机的9090端口：
```shell?linenums
$curl 10.11.36.15:9090
<pre>
<a href="proxy">proxy</a>
<a href="proxy.go">proxy.go</a>
<a href="testfileserver">testfileserver</a>
<a href="testfileserver.go">testfileserver.go</a>
</pre>
```
成功获得file list！
proxy的输出日志：
```shell?linenums
2017/01/11 17:26:16 accept conn &{{0xc4200560e0}}
2017/01/11 17:26:16 dial  172.16.1.1:8080  ok
communication over: error:<nil>
```
由于每个做端口映射的Container都要启动至少一个docker proxy与之配合，一旦运行的container增多，那么docker proxy对资源的消耗将是大大的。因此docker engine在docker 1.7之后提供了完全由iptables DNAT实现的端口映射，无需再启动docker proxy进程。我们只需修改一下docker engine的启动配置即可。

Docker安装完成后，将默认在宿主机系统上增加一些iptables规则，以用于Docker容器和容器之间以及和外界的通信，可以使用iptables-save命令查看。其中nat标上的POSTROUTING链有这么一条规则：
```shell?linenums
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
```

#### Docker端口映射两种方案对比

Docker针对端口映射前后有两种方案，一种是1.7版本之前docker-proxy+iptables DNAT 的方式；另一种则是1.7版本(及之后)提供的完全由iptables DNAT实现的端口映射。不过在目前docker 1.9.1中，前一种方式依旧是默认方式。但是从Docker 1.7版本起，Docker提供了一个配置项：–userland-proxy，以让Docker用户决定是否启用docker-proxy，默认为true，即启用docker-proxy。本文续前文，继续探讨使用端口映射时Docker容器网络的通信流程。

本文中的实验环境：docker 1.9.1，ubuntu 12.04宿主机，docker image基于官方ubuntu 14.04 image做的一些软件安装。

（1）–userland-proxy=true(defaut)的情况下端口映射
我们首先在实验环境下采用默认的方式进行端口映射，即–userland-proxy=true。

我们来建立一个 新container – container3(172.17.0.4)，实现了0.0.0.0:12580 -> container3:12580。
```shell?linenums
$docker run -it --name container3 -p 12580:12580 dockernetworking/ubuntu:14.04 /bin/bash
```
这个命令执行后，iptables增加了三条rules：
```shell?linenums
filter forward链:
Chain DOCKER (1 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 ACCEPT     tcp  --  !docker0 docker0  0.0.0.0/0            172.17.0.4           tcp dpt:12580

nat output链:
Chain DOCKER (1 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 DNAT       tcp  --  !docker0 *       0.0.0.0/0            0.0.0.0/0            tcp dpt:12580 to:172.17.0.4:12580

nat postrouting链：

Chain POSTROUTING (policy ACCEPT 24 packets, 1472 bytes)
 pkts bytes target     prot opt in     out     source               destination
    0     0 MASQUERADE  tcp  --  *      *       172.17.0.4           172.17.0.4  
```
我们可以看到了一个DNAT target，是在nat output链中，这个是一个关键点。同样是考虑到调试的方便，在这新增的rules前面，增加LOG target，新的iptables导出内容为：
```shell?linenums
iptables.portmap.stage1.rules

# Generated by iptables-save v1.4.12 on Fri Jan 15 15:31:06 2016
*raw
: PREROUTING ACCEPT [5737658:60554342802]
:OUTPUT ACCEPT [4294004:56674784720]
-A PREROUTING -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-RawPrerouting:" --log-level 7
-A PREROUTING -p tcp -m tcp --sport 12580 -j LOG --log-prefix "[TonyBai]-RawPrerouting:" --log-level 7
-A OUTPUT -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-RawOutput:" --log-level 7
-A OUTPUT -p tcp -m tcp --sport 12580 -j LOG --log-prefix "[TonyBai]-RawOutput:" --log-level 7
COMMIT
# Completed on Fri Jan 15 15:31:06 2016
# Generated by iptables-save v1.4.12 on Fri Jan 15 15:31:06 2016
*filter
:INPUT ACCEPT [4444190:53498587744]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [4292173:56674165678]
: DOCKER - [0:0]
:FwdId0Od0 - [0:0]
:FwdId0Ond0 - [0:0]
:FwdOd0 - [0:0]
-A INPUT ! -i lo -p icmp -j LOG --log-prefix "[TonyBai]-EnterFilterInput:" --log-level 7
-A INPUT -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-FilterInput:" --log-level 7
-A INPUT -p tcp -m tcp --sport 12580 -j LOG --log-prefix "[TonyBai]-FilterInput:" --log-level 7
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j FwdOd0
-A FORWARD -i docker0 ! -o docker0 -j FwdId0Ond0
-A FORWARD -i docker0 -o docker0 -j FwdId0Od0
-A OUTPUT ! -s 127.0.0.1/32 -p icmp -j LOG --log-prefix "[TonyBai]-EnterFilterOutput:" --log-level 7
-A OUTPUT -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-FilterOutput:" --log-level 7
-A OUTPUT -p tcp -m tcp --sport 12580 -j LOG --log-prefix "[TonyBai]-FilterOutput:" --log-level 7
-A DOCKER -d 172.17.0.4/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-PortmapFowardDocker:" --log-level 7
-A DOCKER -d 172.17.0.4/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 12580 -j ACCEPT
-A FwdId0Od0 -i docker0 -o docker0 -j LOG --log-prefix "[TonyBai]-FwdId0Od0:" --log-level 7
-A FwdId0Od0 -i docker0 -o docker0 -j ACCEPT
-A FwdId0Ond0 -i docker0 ! -o docker0 -j LOG --log-prefix "[TonyBai]-FwdId0Ond0:" --log-level 7
-A FwdId0Ond0 -i docker0 ! -o docker0 -j ACCEPT
-A FwdOd0 -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j LOG --log-prefix "[TonyBai]-FwdOd0:" --log-level 7
-A FwdOd0 -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
COMMIT
# Completed on Fri Jan 15 15:31:06 2016
# Generated by iptables-save v1.4.12 on Fri Jan 15 15:31:06 2016
*nat
: PREROUTING ACCEPT [24690:5091417]
:INPUT ACCEPT [10942:2271167]
:OUTPUT ACCEPT [7756:523318]
: POSTROUTING ACCEPT [7759:523498]
: DOCKER - [0:0]
:LogNatPostRouting - [0:0]
-A PREROUTING -p icmp -j LOG --log-prefix "[TonyBai]-Enter iptables:" --log-level 7
-A PREROUTING -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-NatPrerouting:" --log-level 7
-A PREROUTING -p tcp -m tcp --sport 12580 -j LOG --log-prefix "[TonyBai]-NatPrerouting:" --log-level 7
-A INPUT ! -i lo -p icmp -j LOG --log-prefix "[TonyBai]-EnterNatInput:" --log-level 7
-A INPUT -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-NatInput:" --log-level 7
-A INPUT -p tcp -m tcp --sport 12580 -j LOG --log-prefix "[TonyBai]-NatInput:" --log-level 7
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j LogNatPostRouting
-A POSTROUTING -s 172.17.0.4/32 -d 172.17.0.4/32 -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-PortmapNatPostRouting:" --log-level 7
-A POSTROUTING -s 172.17.0.4/32 -d 172.17.0.4/32 -p tcp -m tcp --dport 12580 -j MASQUERADE
-A DOCKER ! -i docker0 -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-PortmapNatOutputDocker:" --log-level 7
-A DOCKER ! -i docker0 -p tcp -m tcp --dport 12580 -j DNAT --to-destination 172.17.0.4:12580
-A LogNatPostRouting -s 172.17.0.0/16 ! -o docker0 -j LOG --log-prefix "[TonyBai]-NatPostRouting:" --log-level 7
-A LogNatPostRouting -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
COMMIT
# Completed on Fri Jan 15 15:31:06 2016
```
另外我们可以查看到宿主机中多了一个进程，这就是前面所说的docker-proxy，每增加一个端口映射，宿主机就会多出一个docker-proxy进程：
```shell?linenums
root      5742  2113  0 08:48 ?        00:00:00 docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 12580 -container-ip 172.17.0.4 -container-port 12580
```
1、从10.10.126.187访问宿主机(10.10.126.101)的12580端口
10.10.126.187是与101在同一直连网路的主机，我们在其上执行telnet 10.10.126.101 12580。如果container3中有server在监听12580，则建立连接和数据通信(发送一个hello)的过程如下。

【187到101的tcp握手sync包】

101从eth0网卡收到目的地址是自己的sync数据包：
```shell?linenums
Jan 15 16:04:54 pc-baim kernel: [28410.162828] [TonyBai]-RawPrerouting:IN=eth0 OUT= MAC=2c:59:e5:01:98:28:00:19:bb:5e:0a:86:08:00 SRC=10.10.126.187 DST=10.10.126.101 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=32617 DF PROTO=TCP SPT=33250 DPT=12580 WINDOW=5840 RES=0x00 SYN URGP=0
Jan 15 16:04:54 pc-baim kernel: [28410.162862] [TonyBai]-NatPrerouting:IN=eth0 OUT= MAC=2c:59:e5:01:98:28:00:19:bb:5e:0a:86:08:00 SRC=10.10.126.187 DST=10.10.126.101 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=32617 DF PROTO=TCP SPT=33250 DPT=12580 WINDOW=5840 RES=0x00 SYN URGP=0
```
由于目的地址就是自己，因此在iptables中走input chain将数据包发给user层：
```shell?linenums
Jan 15 16:04:54 pc-baim kernel: [28410.162885] [TonyBai]-FilterInput:IN=eth0 OUT= MAC=2c:59:e5:01:98:28:00:19:bb:5e:0a:86:08:00 SRC=10.10.126.187 DST=10.10.126.101 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=32617 DF PROTO=TCP SPT=33250 DPT=12580 WINDOW=5840 RES=0x00 SYN URGP=0
Jan 15 16:04:54 pc-baim kernel: [28410.162900] [TonyBai]-NatInput:IN=eth0 OUT= MAC=2c:59:e5:01:98:28:00:19:bb:5e:0a:86:08:00 SRC=10.10.126.187 DST=10.10.126.101 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=32617 DF PROTO=TCP SPT=33250 DPT=12580 WINDOW=5840 RES=0x00 SYN URGP=0
```
【101回复ack sync包】

101上的用户层是docker-proxy在监听12580端口，当收到sync后，会回复ack sync。由于是user空间自产包，路由后走output链。
```shell?linenums
Jan 15 16:04:54 pc-baim kernel: [28410.162933] [TonyBai]-RawOutput:IN= OUT=eth0 SRC=10.10.126.101 DST=10.10.126.187 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=12580 DPT=33250 WINDOW=28960 RES=0x00 ACK SYN URGP=0
Jan 15 16:04:54 pc-baim kernel: [28410.162948] [TonyBai]-FilterOutput:IN= OUT=eth0 SRC=10.10.126.101 DST=10.10.126.187 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=12580 DPT=33250 WINDOW=28960 RES=0x00 ACK SYN URGP=0
```
【187回复ack，101与187握手完成】

187回复握手过程最后的一个ack。这个过程与sync类似：
```shell?linenums
Jan 15 16:04:54 pc-baim kernel: [28410.163397] [TonyBai]-RawPrerouting:IN=eth0 OUT= MAC=2c:59:e5:01:98:28:00:19:bb:5e:0a:86:08:00 SRC=10.10.126.187 DST=10.10.126.101 LEN=52 TOS=0x10 PREC=0x00 TTL=64 ID=32618 DF PROTO=TCP SPT=33250 DPT=12580 WINDOW=92 RES=0x00 ACK URGP=0
Jan 15 16:04:54 pc-baim kernel: [28410.163437] [TonyBai]-FilterInput:IN=eth0 OUT= MAC=2c:59:e5:01:98:28:00:19:bb:5e:0a:86:08:00 SRC=10.10.126.187 DST=10.10.126.101 LEN=52 TOS=0x10 PREC=0x00 TTL=64 ID=32618 DF PROTO=TCP SPT=33250 DPT=12580 WINDOW=92 RES=0x00 ACK URGP=0
```
重点是接下来发生的事情：101上的docker-proxy向container3上的server程序建立tcp连接！

【host向container3发送sync】
```shell?linenums
Jan 15 16:04:54 pc-baim kernel: [28410.163863] [TonyBai]-RawOutput:IN= OUT=docker0 SRC=172.17.0.1 DST=172.17.0.4 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=5768 DF PROTO=TCP SPT=43771 DPT=12580 WINDOW=29200 RES=0x00 SYN URGP=0
Jan 15 16:04:54 pc-baim kernel: [28410.163901] [TonyBai]-FilterOutput:IN= OUT=docker0 SRC=172.17.0.1 DST=172.17.0.4 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=5768 DF PROTO=TCP SPT=43771 DPT=12580 WINDOW=29200 RES=0x00 SYN URGP=0
```
我们看到SYN数据包源地址用的是172.17.0.1，不知是否是docker-proxy内部有意选择了网桥的ip。由于是user层发出的包，于是走iptables output链。

【container3回复ack sync】

container3回复ack sync，目的地址是172.17.0.1，host从docker0网卡收到ack sync数据，路由后发现是发给自己的包，于是走input chain.
```shell?linenums
Jan 15 16:04:54 pc-baim kernel: [28410.164000] [TonyBai]-RawPrerouting:IN=docker0 OUT= PHYSIN=vethf0cc298 MAC=02:42:23:39:fd:f5:02:42:ac:11:00:04:08:00 SRC=172.17.0.4 DST=172.17.0.1 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=12580 DPT=43771 WINDOW=28960 RES=0x00 ACK SYN URGP=0
Jan 15 16:04:54 pc-baim kernel: [28410.164026] [TonyBai]-FilterInput:IN=docker0 OUT= PHYSIN=vethf0cc298 MAC=02:42:23:39:fd:f5:02:42:ac:11:00:04:08:00 SRC=172.17.0.4 DST=172.17.0.1 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=12580 DPT=43771 WINDOW=28960 RES=0x00 ACK SYN URGP=0
```
【host回复ack，host与container3握手完成】

host回复握手过程最后的一个ack。user空间自产数据包，于是走output chain：
```shell?linenums
Jan 15 16:04:54 pc-baim kernel: [28410.164049] [TonyBai]-RawOutput:IN= OUT=docker0 SRC=172.17.0.1 DST=172.17.0.4 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=5769 DF PROTO=TCP SPT=43771 DPT=12580 WINDOW=229 RES=0x00 ACK URGP=0
Jan 15 16:04:54 pc-baim kernel: [28410.164058] [TonyBai]-FilterOutput:IN= OUT=docker0 SRC=172.17.0.1 DST=172.17.0.4 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=5769 DF PROTO=TCP SPT=43771 DPT=12580 WINDOW=229 RES=0x00 ACK URGP=0
```
【187 在已经建立的连接上发送”hello”】

187发送hello to host，docker-proxy收到hello数据：
```shell?linenums
Jan 15 16:04:58 pc-baim kernel: [28413.840854] [TonyBai]-RawPrerouting:IN=eth0 OUT= MAC=2c:59:e5:01:98:28:00:19:bb:5e:0a:86:08:00 SRC=10.10.126.187 DST=10.10.126.101 LEN=59 TOS=0x10 PREC=0x00 TTL=64 ID=32619 DF PROTO=TCP SPT=33250 DPT=12580 WINDOW=92 RES=0x00 ACK PSH URGP=0
Jan 15 16:04:58 pc-baim kernel: [28413.840874] [TonyBai]-FilterInput:IN=eth0 OUT= MAC=2c:59:e5:01:98:28:00:19:bb:5e:0a:86:08:00 SRC=10.10.126.187 DST=10.10.126.101 LEN=59 TOS=0x10 PREC=0x00 TTL=64 ID=32619 DF PROTO=TCP SPT=33250 DPT=12580 WINDOW=92 RES=0x00 ACK PSH URGP=0
```
【host返回 ack push】
```shell?linenums
Jan 15 16:04:58 pc-baim kernel: [28413.840893] [TonyBai]-RawOutput:IN= OUT=eth0 SRC=10.10.126.101 DST=10.10.126.187 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=22415 DF PROTO=TCP SPT=12580 DPT=33250 WINDOW=227 RES=0x00 ACK URGP=0
Jan 15 16:04:58 pc-baim kernel: [28413.840902] [TonyBai]-FilterOutput:IN= OUT=eth0 SRC=10.10.126.101 DST=10.10.126.187 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=22415 DF PROTO=TCP SPT=12580 DPT=33250 WINDOW=227 RES=0x00 ACK URGP=0
```
接下来，docker-proxy将hello从已有连接上转发给container3。

【host转发hello到container3】
```shell?linenums
Jan 15 16:04:58 pc-baim kernel: [28413.841000] [TonyBai]-RawOutput:IN= OUT=docker0 SRC=172.17.0.1 DST=172.17.0.4 LEN=59 TOS=0x00 PREC=0x00 TTL=64 ID=5770 DF PROTO=TCP SPT=43771 DPT=12580 WINDOW=229 RES=0x00 ACK PSH URGP=0
Jan 15 16:04:58 pc-baim kernel: [28413.841026] [TonyBai]-FilterOutput:IN= OUT=docker0 SRC=172.17.0.1 DST=172.17.0.4 LEN=59 TOS=0x00 PREC=0x00 TTL=64 ID=5770 DF PROTO=TCP SPT=43771 DPT=12580 WINDOW=229 RES=0x00 ACK PSH URGP=0
```
【container3回复ack 】
```shell?linenums
Jan 15 16:04:58 pc-baim kernel: [28413.841101] [TonyBai]-RawPrerouting:IN=docker0 OUT= PHYSIN=vethf0cc298 MAC=02:42:23:39:fd:f5:02:42:ac:11:00:04:08:00 SRC=172.17.0.4 DST=172.17.0.1 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=61139 DF PROTO=TCP SPT=12580 DPT=43771 WINDOW=227 RES=0x00 ACK URGP=0
Jan 15 16:04:58 pc-baim kernel: [28413.841119] [TonyBai]-FilterInput:IN=docker0 OUT= PHYSIN=vethf0cc298 MAC=02:42:23:39:fd:f5:02:42:ac:11:00:04:08:00 SRC=172.17.0.4 DST=172.17.0.1 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=61139 DF PROTO=TCP SPT=12580 DPT=43771 WINDOW=227 RES=0x00 ACK URGP=0
```
通信过程到此结束。通过这个过程，我们至少了解到两点：

1、docker-proxy将外部建立在host:12580上的连接上的数据转发到container中，反之亦然，如果container 通过与host已经建立的连接向外发送数据，docker-proxy也会将数据转发给187。
2、通过iptables log输出我们可以看到：为了port map而添加的DNAT和MASQUERADE 并没有被匹配到，也就是说在这个过程中并没有用到DNAT，而是完全依靠docker-proxy做的4层代理。

2、从宿主机上访问10.10.126.101:12580
我们在宿主机本机上访问10.10.126.101:12580，看看这个通信过程与上面的是否有差异。

【与本机12580端口建立连接，发送sync包】

由于是user层发送数据包，因此走iptables output链。
```shell?linenums
Jan 15 16:40:15 pc-baim kernel: [30532.594545] [TonyBai]-RawOutput:IN= OUT=lo SRC=10.10.126.101 DST=10.10.126.101 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=53747 DF PROTO=TCP SPT=48039 DPT=12580 WINDOW=43690 RES=0x00 SYN URGP=0
```
在output链上，匹配到nat output上的规则：
```shell?linenums
Chain DOCKER (1 references)
 pkts bytes target     prot opt in     out     source               destination
    1    60 LOG        tcp  --  !docker0 *       0.0.0.0/0            0.0.0.0/0            tcp dpt:12580 LOG flags 0 level 7 prefix "[TonyBai]-PortmapNatOutputDoc"
    1    60 DNAT       tcp  --  !docker0 *       0.0.0.0/0            0.0.0.0/0            tcp dpt:12580 to:172.17.0.4:12580
	```
于是这里将做一个DNAT，数据包的目的地址10.10.126.101被替换为172.17.0.4。
```shell?linenums
Jan 15 16:40:15 pc-baim kernel: [30532.594561] [TonyBai]-PortmapNatOutputDoc IN= OUT=lo SRC=10.10.126.101 DST=10.10.126.101 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=53747 DF PROTO=TCP SPT=48039 DPT=12580 WINDOW=43690 RES=0x00 SYN URGP=0

Jan 15 16:40:15 pc-baim kernel: [30532.594572] [TonyBai]-FilterOutput:IN= OUT=lo SRC=10.10.126.101 DST=172.17.0.4 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=53747 DF PROTO=TCP SPT=48039 DPT=12580 WINDOW=43690 RES=0x00 SYN URGP=0
```
DNAT后，将按照目的地址做一个重新路由：叫实际路由。消息实际重定向到docker0进行封包发送，sync包直接进入到container3 中。

【container3发送ack sync包】

docker0出来的ack sync 通过input chain送到user空间。这块应该由一个自动un-DNAT，将172.17.0.4自动转回10.10.126.101，但通过iptables日志无法确认这点。
```shell?linenums
Jan 15 16:40:15 pc-baim kernel: [30532.594615] [TonyBai]-RawPrerouting:IN=docker0 OUT= PHYSIN=vethf0cc298 MAC=02:42:23:39:fd:f5:02:42:ac:11:00:04:08:00 SRC=172.17.0.4 DST=10.10.126.101 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=12580 DPT=48039 WINDOW=28960 RES=0x00 ACK SYN URGP=0
Jan 15 16:40:15 pc-baim kernel: [30532.594624] [TonyBai]-FilterInput:IN=docker0 OUT= PHYSIN=vethf0cc298 MAC=02:42:23:39:fd:f5:02:42:ac:11:00:04:08:00 SRC=172.17.0.4 DST=10.10.126.101 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=12580 DPT=48039 WINDOW=28960 RES=0x00 ACK SYN URGP=0
```
【host发送ack，完成握手】

host回复ack。user层自产包，走output链，看rawoutput，dst依旧是126.101(telnet自然不应该知道 172.17.0.4的存在)，但是filter output 前，iptables对该地址自动做了dnat，无需重新进入到nat output链，因为之前已经进过了。在filter output中，我们看到dst ip已经变成了container3的ip地址：
```shell?linenums
Jan 15 16:40:15 pc-baim kernel: [30532.594637] [TonyBai]-RawOutput:IN= OUT=lo SRC=10.10.126.101 DST=10.10.126.101 LEN=52 TOS=0x10 PREC=0x00 TTL=64 ID=53748 DF PROTO=TCP SPT=48039 DPT=12580 WINDOW=342 RES=0x00 ACK URGP=0
Jan 15 16:40:15 pc-baim kernel: [30532.594643] [TonyBai]-FilterOutput:IN= OUT=lo SRC=10.10.126.101 DST=172.17.0.4 LEN=52 TOS=0x10 PREC=0x00 TTL=64 ID=53748 DF PROTO=TCP SPT=48039 DPT=12580 WINDOW=342 RES=0x00 ACK URGP=0
```
【host发送hello】

这个过程同上，不赘述。
```shell?linenums
Jan 15 16:40:18 pc-baim kernel: [30535.344921] [TonyBai]-RawOutput:IN= OUT=lo SRC=10.10.126.101 DST=10.10.126.101 LEN=59 TOS=0x10 PREC=0x00 TTL=64 ID=53749 DF PROTO=TCP SPT=48039 DPT=12580 WINDOW=342 RES=0x00 ACK PSH URGP=0
Jan 15 16:40:18 pc-baim kernel: [30535.344956] [TonyBai]-FilterOutput:IN= OUT=lo SRC=10.10.126.101 DST=172.17.0.4 LEN=59 TOS=0x10 PREC=0x00 TTL=64 ID=53749 DF PROTO=TCP SPT=48039 DPT=12580 WINDOW=342 RES=0x00 ACK PSH URGP=0
```
【container回复ack】

不赘述。
```shell?linenums
Jan 15 16:40:18 pc-baim kernel: [30535.345027] [TonyBai]-RawPrerouting:IN=docker0 OUT= PHYSIN=vethf0cc298 MAC=02:42:23:39:fd:f5:02:42:ac:11:00:04:08:00 SRC=172.17.0.4 DST=10.10.126.101 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=43021 DF PROTO=TCP SPT=12580 DPT=48039 WINDOW=227 RES=0x00 ACK URGP=0
Jan 15 16:40:18 pc-baim kernel: [30535.345056] [TonyBai]-FilterInput:IN=docker0 OUT= PHYSIN=vethf0cc298 MAC=02:42:23:39:fd:f5:02:42:ac:11:00:04:08:00 SRC=172.17.0.4 DST=10.10.126.101 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=43021 DF PROTO=TCP SPT=12580 DPT=48039 WINDOW=227 RES=0x00 ACK URGP=0
```
从这个过程可以看到，在宿主机上访问container的映射端口，通信流程不走docker-proxy，而是直接通过output 的dnat将数据包被直接转给container中的server程序。

3、container to container
在container1中telnet 10.10.126.101 12580会发生什么呢？这里就不长篇大论的列log了，直接给出结论：通过docker-proxy转发，因为不满足nat output中DNAT的匹配条件。

二、在–userland-proxy=false的情况下
我们修改了一下/etc/default/docker配置，为DOCKER_OPTS增加一个option: –userland-proxy=false。
```shell?linenums
DOCKER_OPTS="--dns 8.8.8.8 --dns 8.8.4.4 --userland-proxy=false"
```
重启docker daemon并清理iptables规则(-F)，并启动做端口映射的container3。启动后，你会发现之前的docker-proxy并没有出现在启动进程列表中，iptables的规则与–userland-proxy=true时也有所不同：
```shell?linenums
$ sudo iptables -nL -v
Chain INPUT (policy ACCEPT 1645 packets, 368K bytes)
 pkts bytes target     prot opt in     out     source               destination

Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
    0     0 DOCKER     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0
    0     0 ACCEPT     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
    0     0 ACCEPT     all  --  docker0 !docker0  0.0.0.0/0            0.0.0.0/0
    0     0 ACCEPT     all  --  docker0 docker0  0.0.0.0/0            0.0.0.0/0

Chain OUTPUT (policy ACCEPT 263 packets, 134K bytes)
 pkts bytes target     prot opt in     out     source               destination

Chain DOCKER (1 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 ACCEPT     tcp  --  !docker0 docker0  0.0.0.0/0            172.17.0.4           tcp dpt:12580

$ sudo iptables -t nat -nL -v
Chain PREROUTING (policy ACCEPT 209 packets, 65375 bytes)
 pkts bytes target     prot opt in     out     source               destination
   71 49357 DOCKER     all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL

Chain INPUT (policy ACCEPT 98 packets, 39060 bytes)
 pkts bytes target     prot opt in     out     source               destination

Chain OUTPUT (policy ACCEPT 34 packets, 2096 bytes)
 pkts bytes target     prot opt in     out     source               destination
   21  1302 DOCKER     all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL

Chain POSTROUTING (policy ACCEPT 34 packets, 2096 bytes)
 pkts bytes target     prot opt in     out     source               destination
    0     0 MASQUERADE  all  --  *      docker0  0.0.0.0/0            0.0.0.0/0            ADDRTYPE match src-type LOCAL
    0     0 MASQUERADE  all  --  *      !docker0  172.17.0.0/16        0.0.0.0/0
    0     0 MASQUERADE  tcp  --  *      *       172.17.0.4           172.17.0.4           tcp dpt:12580

Chain DOCKER (2 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:12580 to:172.17.0.4:12580
```
可以看到nat表中prerouting链增加了target为DOCKER链的规则，并且Docker链中对dnat的匹配条件也放开了，只要是dst-type是LOCAL的，dport=12580的，都将ip映射为172.17.0.4。

由于iptables的规则有所变化，因此因此我的log target的匹配条件也该调整一下了，调整后的iptables为：
```shell?linenums
iptables.portmap.stage1.tmp.rules

# Generated by iptables-save v1.4.12 on Mon Jan 18 09:06:06 2016
*mangle
: POSTROUTING ACCEPT [0:0]
-A POSTROUTING -o docker0 -m addrtype --src-type LOCAL -j LOG --log-prefix "[TonyBai]-manglepost1" --log-level 7
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j LOG --log-prefix "[TonyBai]-manglepost2" --log-level 7
-A POSTROUTING -s 172.17.0.4/32 -d 172.17.0.4/32 -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-manglepost3" --log-level 7
COMMIT

*raw
: PREROUTING ACCEPT [1008742:377375989]
:OUTPUT ACCEPT [426678:274235692]
-A PREROUTING -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-RawPrerouting:" --log-level 7
-A PREROUTING -p tcp -m tcp --sport 12580 -j LOG --log-prefix "[TonyBai]-RawPrerouting:" --log-level 7
-A OUTPUT -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-RawOutput:" --log-level 7
-A OUTPUT -p tcp -m tcp --sport 12580 -j LOG --log-prefix "[TonyBai]-RawOutput:" --log-level 7
COMMIT
# Completed on Mon Jan 18 09:06:06 2016
# Generated by iptables-save v1.4.12 on Mon Jan 18 09:06:06 2016
*filter
:INPUT ACCEPT [187016:64478647]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [81342:51955911]
: DOCKER - [0:0]
:FwdId0Od0 - [0:0]
:FwdId0Ond0 - [0:0]
:FwdOd0 - [0:0]
-A INPUT ! -i lo -p icmp -j LOG --log-prefix "[TonyBai]-EnterFilterInput:" --log-level 7
-A INPUT -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-FilterInput:" --log-level 7
-A INPUT -p tcp -m tcp --sport 12580 -j LOG --log-prefix "[TonyBai]-FilterInput:" --log-level 7
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j FwdOd0
-A FORWARD -i docker0 ! -o docker0 -j FwdId0Ond0
-A FORWARD -i docker0 -o docker0 -j FwdId0Od0
-A OUTPUT ! -s 127.0.0.1/32 -p icmp -j LOG --log-prefix "[TonyBai]-EnterFilterOutput:" --log-level 7
-A OUTPUT -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-FilterOutput:" --log-level 7
-A OUTPUT -p tcp -m tcp --sport 12580 -j LOG --log-prefix "[TonyBai]-FilterOutput:" --log-level 7
-A DOCKER -d 172.17.0.4/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-PortmapFowardDocker" --log-level 7
-A DOCKER -d 172.17.0.4/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 12580 -j ACCEPT
-A FwdId0Od0 -i docker0 -o docker0 -j LOG --log-prefix "[TonyBai]-FwdId0Od0:" --log-level 7
-A FwdId0Od0 -i docker0 -o docker0 -j ACCEPT
-A FwdId0Ond0 -i docker0 ! -o docker0 -j LOG --log-prefix "[TonyBai]-FwdId0Ond0:" --log-level 7
-A FwdId0Ond0 -i docker0 ! -o docker0 -j ACCEPT
-A FwdOd0 -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j LOG --log-prefix "[TonyBai]-FwdOd0:" --log-level 7
-A FwdOd0 -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
COMMIT
# Completed on Mon Jan 18 09:06:06 2016
# Generated by iptables-save v1.4.12 on Mon Jan 18 09:06:06 2016
*nat
: PREROUTING ACCEPT [34423:7014094]
:INPUT ACCEPT [9475:1880078]
:OUTPUT ACCEPT [3524:218202]
: POSTROUTING ACCEPT [3508:217098]
: DOCKER - [0:0]
:LogNatPostRouting1 - [0:0]
:LogNatPostRouting2 - [0:0]
:LogNatPostRouting3 - [0:0]
-A PREROUTING -p icmp -j LOG --log-prefix "[TonyBai]-Enter iptables:" --log-level 7
-A PREROUTING -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-NatPrerouting:" --log-level 7
-A PREROUTING -p tcp -m tcp --sport 12580 -j LOG --log-prefix "[TonyBai]-NatPrerouting:" --log-level 7
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A INPUT ! -i lo -p icmp -j LOG --log-prefix "[TonyBai]-EnterNatInput:" --log-level 7
-A INPUT -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-NatInput:" --log-level 7
-A INPUT -p tcp -m tcp --sport 12580 -j LOG --log-prefix "[TonyBai]-NatInput:" --log-level 7
-A OUTPUT -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -p tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-NatPostrouteEnter" --log-level 7
-A POSTROUTING -p tcp --sport 12580 -j LOG --log-prefix "[TonyBai]-NatPostrouteEnter" --log-level 7
-A POSTROUTING -o docker0 -m addrtype --src-type LOCAL -j LogNatPostRouting1
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j LogNatPostRouting2
-A POSTROUTING -s 172.17.0.4/32 -d 172.17.0.4/32 -p tcp -m tcp --dport 12580 -j LogNatPostRouting3
-A DOCKER -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-PortmapNatPrerouting" --log-level 7
-A DOCKER -p tcp -m tcp --dport 12580 -j DNAT --to-destination 172.17.0.4:12580
-A LogNatPostRouting1 -o docker0 -m addrtype --src-type LOCAL -j LOG --log-prefix "[TonyBai]-NatPost1" --log-level 7
-A LogNatPostRouting1 -o docker0 -m addrtype --src-type LOCAL -j MASQUERADE
-A LogNatPostRouting2 -s 172.17.0.0/16 ! -o docker0 -j LOG --log-prefix "[TonyBai]-NatPost2" --log-level 7
-A LogNatPostRouting2 -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A LogNatPostRouting3 -s 172.17.0.4/32 -d 172.17.0.4/32 -p tcp -m tcp --dport 12580 -j LOG --log-prefix "[TonyBai]-NatPost3" --log-level 7
-A LogNatPostRouting3 -s 172.17.0.4/32 -d 172.17.0.4/32 -p tcp -m tcp --dport 12580 -j MASQUERADE
COMMIT
# Completed on Mon Jan 18 09:06:06 2016
```
接下来，我们按照上面的方法再做一遍实验例子，看看通信流程有何不同。这次我们将187主机换为10.10.105.71，其他无差别。

1、 在71上telnet 10.10.126.101 12580
宿主机从eth0接口收到syn，nat prerouting中做DNAT。路由后，通过forward链转发到docker0：
```shell?linenums
Jan 18 13:35:55 pc-baim kernel: [278835.389225] [TonyBai]-RawPrerouting:IN=eth0 OUT= MAC=2c:59:e5:01:98:28:00:23:89:7d:b6:b1:08:00 SRC=10.10.105.71 DST=10.10.126.101 LEN=60 TOS=0x10 PREC=0x00 TTL=63 ID=61480 DF PROTO=TCP SPT=41502 DPT=12580 WINDOW=29200 RES=0x00 SYN URGP=0
Jan 18 13:35:55 pc-baim kernel: [278835.389275] [TonyBai]-NatPrerouting:IN=eth0 OUT= MAC=2c:59:e5:01:98:28:00:23:89:7d:b6:b1:08:00 SRC=10.10.105.71 DST=10.10.126.101 LEN=60 TOS=0x10 PREC=0x00 TTL=63 ID=61480 DF PROTO=TCP SPT=41502 DPT=12580 WINDOW=29200 RES=0x00 SYN URGP=0
Jan 18 13:35:55 pc-baim kernel: [278835.389290] [TonyBai]-PortmapNatPreroutinIN=eth0 OUT= MAC=2c:59:e5:01:98:28:00:23:89:7d:b6:b1:08:00 SRC=10.10.105.71 DST=10.10.126.101 LEN=60 TOS=0x10 PREC=0x00 TTL=63 ID=61480 DF PROTO=TCP SPT=41502 DPT=12580 WINDOW=29200 RES=0x00 SYN URGP=0
Jan 18 13:35:55 pc-baim kernel: [278835.389326] [TonyBai]-PortmapFowardDockerIN=eth0 OUT=docker0 MAC=2c:59:e5:01:98:28:00:23:89:7d:b6:b1:08:00 SRC=10.10.105.71 DST=172.17.0.4 LEN=60 TOS=0x10 PREC=0x00 TTL=62 ID=61480 DF PROTO=TCP SPT=41502 DPT=12580 WINDOW=29200 RES=0x00 SYN URGP=0
Jan 18 13:35:55 pc-baim kernel: [278835.389339] [TonyBai]-NatPostrouteEnterIN= OUT=docker0 SRC=10.10.105.71 DST=172.17.0.4 LEN=60 TOS=0x10 PREC=0x00 TTL=62 ID=61480 DF PROTO=TCP SPT=41502 DPT=12580 WINDOW=29200 RES=0x00 SYN URGP=0
```
接下来从docker0网卡收到container3的ack syn应答，在从eth0转发出去前自动un-DNAT， src ip从172.17.0.4变为101.0126.101，但这个在日志中看不出来。
```shell?linenums
Jan 18 13:35:55 pc-baim kernel: [278835.389496] [TonyBai]-RawPrerouting:IN=docker0 OUT= PHYSIN=veth0d66af2 MAC=02:42:23:39:fd:f5:02:42:ac:11:00:04:08:00 SRC=172.17.0.4 DST=10.10.105.71 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=12580 DPT=41502 WINDOW=28960 RES=0x00 ACK SYN URGP=0
Jan 18 13:35:55 pc-baim kernel: [278835.389519] [TonyBai]-FwdId0Ond0:IN=docker0 OUT=eth0 PHYSIN=veth0d66af2 MAC=02:42:23:39:fd:f5:02:42:ac:11:00:04:08:00 SRC=172.17.0.4 DST=10.10.105.71 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=0 DF PROTO=TCP SPT=12580 DPT=41502 WINDOW=28960 RES=0x00 ACK SYN URGP=0
Jan 18 13:35:55 pc-baim kernel: [278835.389528] [TonyBai]-manglepost2IN= OUT=eth0 PHYSIN=veth0d66af2 SRC=172.17.0.4 DST=10.10.105.71 LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=0 DF PROTO=TCP SPT=12580 DPT=41502 WINDOW=28960 RES=0x00 ACK SYN URGP=0
```
回送ack，这回无需再匹配natprerouting链，前面进过链一次，后续自动进行DNAT：
```shell?linenums
Jan 18 13:35:55 pc-baim kernel: [278835.390079] [TonyBai]-RawPrerouting:IN=eth0 OUT= MAC=2c:59:e5:01:98:28:00:23:89:7d:b6:b1:08:00 SRC=10.10.105.71 DST=10.10.126.101 LEN=52 TOS=0x10 PREC=0x00 TTL=63 ID=61481 DF PROTO=TCP SPT=41502 DPT=12580 WINDOW=229 RES=0x00 ACK URGP=0
Jan 18 13:35:55 pc-baim kernel: [278835.390149] [TonyBai]-PortmapFowardDockerIN=eth0 OUT=docker0 MAC=2c:59:e5:01:98:28:00:23:89:7d:b6:b1:08:00 SRC=10.10.105.71 DST=172.17.0.4 LEN=52 TOS=0x10 PREC=0x00 TTL=62 ID=61481 DF PROTO=TCP SPT=41502 DPT=12580 WINDOW=229 RES=0x00 ACK URGP=0
```
这次我们看到，在这种方式下，外部流量也是通过DNAT方式导入到container中的。

2、在宿主机上 telnet 10.10.126.101 12580
telnet发起tcp握手，syn包进入output链，匹配到nat output规则，做DNAT。目的ip转换为172.17.0.4。注意继续向下，我们看iptables匹配到了NatPost1，也就是规则：
```shell?linenums
-A LogNatPostRouting1 -o docker0 -m addrtype --src-type LOCAL -j MASQUERADE
```
即将源地址伪装为出口网卡docker0的当前地址：172.0.0.1。于是实际上进入到container3的syn数据包的源地址为172.0.0.1，目的地址：172.0.0.4。
```shell?linenums
Jan 18 13:49:43 pc-baim kernel: [279663.426497] [TonyBai]-RawOutput:IN= OUT=lo SRC=10.10.126.101 DST=10.10.126.101 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=40854 DF PROTO=TCP SPT=52736 DPT=12580 WINDOW=43690 RES=0x00 SYN URGP=0
Jan 18 13:49:43 pc-baim kernel: [279663.426526] [TonyBai]-PortmapNatPreroutinIN= OUT=lo SRC=10.10.126.101 DST=10.10.126.101 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=40854 DF PROTO=TCP SPT=52736 DPT=12580 WINDOW=43690 RES=0x00 SYN URGP=0
Jan 18 13:49:43 pc-baim kernel: [279663.426545] [TonyBai]-FilterOutput:IN= OUT=lo SRC=10.10.126.101 DST=172.17.0.4 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=40854 DF PROTO=TCP SPT=52736 DPT=12580 WINDOW=43690 RES=0x00 SYN URGP=0
Jan 18 13:49:43 pc-baim kernel: [279663.426553] [TonyBai]-manglepost1IN= OUT=docker0 SRC=10.10.126.101 DST=172.17.0.4 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=40854 DF PROTO=TCP SPT=52736 DPT=12580 WINDOW=43690 RES=0x00 SYN URGP=0
Jan 18 13:49:43 pc-baim kernel: [279663.426561] [TonyBai]-NatPostrouteEnterIN= OUT=docker0 SRC=10.10.126.101 DST=172.17.0.4 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=40854 DF PROTO=TCP SPT=52736 DPT=12580 WINDOW=43690 RES=0x00 SYN URGP=0
Jan 18 13:49:43 pc-baim kernel: [279663.426567] [TonyBai]-NatPost1IN= OUT=docker0 SRC=10.10.126.101 DST=172.17.0.4 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=40854 DF PROTO=TCP SPT=52736 DPT=12580 WINDOW=43690 RES=0x00 SYN URGP=0
```
container3返回ack，从宿主机角度来看，相当于从docker0网卡收到ack。我们看到进来的原始数据：dst = 172.17.0.1，这是上面MASQUERADE的作用。在进入input链前，做自动un-SNAT，目的地址由172.17.0.1转换为10.10.126.101。在真正送到user层之前（output链等同的左边同纬度位置），做自动un-DNAT(但在下面日志中看不出来)，src由172.17.0.4变为10.10.126.101。数据包的变换总体次序依次为：即DNAT -> SNAT -> (应答包)un-SNAT -> un-DNAT。
```shell?linenums
Jan 18 13:49:43 pc-baim kernel: [279663.426646] [TonyBai]-RawPrerouting:IN=docker0 OUT= PHYSIN=veth0d66af2 MAC=02:42:23:39:fd:f5:02:42:ac:11:00:04:08:00 SRC=172.17.0.4 DST=172.17.0.1 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=12580 DPT=52736 WINDOW=28960 RES=0x00 ACK SYN URGP=0
Jan 18 13:49:43 pc-baim kernel: [279663.426665] [TonyBai]-FilterInput:IN=docker0 OUT= PHYSIN=veth0d66af2 MAC=02:42:23:39:fd:f5:02:42:ac:11:00:04:08:00 SRC=172.17.0.4 DST=10.10.126.101 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=12580 DPT=52736 WINDOW=28960 RES=0x00 ACK SYN URGP=0
```
宿主机回复ack，握手完成。由于之前走过nat output和post链，因此这里不会再匹配，而是自动DNAT和SNAT：
```shell?linenums
Jan 18 13:49:43 pc-baim kernel: [279663.426690] [TonyBai]-RawOutput:IN= OUT=lo SRC=10.10.126.101 DST=10.10.126.101 LEN=52 TOS=0x10 PREC=0x00 TTL=64 ID=40855 DF PROTO=TCP SPT=52736 DPT=12580 WINDOW=342 RES=0x00 ACK URGP=0
Jan 18 13:49:43 pc-baim kernel: [279663.426707] [TonyBai]-FilterOutput:IN= OUT=lo SRC=10.10.126.101 DST=172.17.0.4 LEN=52 TOS=0x10 PREC=0x00 TTL=64 ID=40855 DF PROTO=TCP SPT=52736 DPT=12580 WINDOW=342 RES=0x00 ACK URGP=0
Jan 18 13:49:43 pc-baim kernel: [279663.426719] [TonyBai]-manglepost1IN= OUT=docker0 SRC=10.10.126.101 DST=172.17.0.4 LEN=52 TOS=0x10 PREC=0x00 TTL=64 ID=40855 DF PROTO=TCP SPT=52736 DPT=12580 WINDOW=342 RES=0x00 ACK URGP=0
```
3、从container1 telnet 10.10.126.101 12580
container1向服务发起tcp连接，宿主机从docker0网卡收到sync包。
```shell?linenums
Jan 18 13:51:10 pc-baim kernel: [279750.806496] [TonyBai]-RawPrerouting:IN=docker0 OUT= PHYSIN=veth44a97d7 MAC=02:42:23:39:fd:f5:02:42:ac:11:00:02:08:00 SRC=172.17.0.2 DST=10.10.126.101 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=31888 DF PROTO=TCP SPT=54408 DPT=12580 WINDOW=29200 RES=0x00 SYN URGP=0
Jan 18 13:51:10 pc-baim kernel: [279750.806519] [TonyBai]-NatPrerouting:IN=docker0 OUT= PHYSIN=veth44a97d7 MAC=02:42:23:39:fd:f5:02:42:ac:11:00:02:08:00 SRC=172.17.0.2 DST=10.10.126.101 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=31888 DF PROTO=TCP SPT=54408 DPT=12580 WINDOW=29200 RES=0x00 SYN URGP=0
Jan 18 13:51:10 pc-baim kernel: [279750.806531] [TonyBai]-PortmapNatPreroutinIN=docker0 OUT= PHYSIN=veth44a97d7 MAC=02:42:23:39:fd:f5:02:42:ac:11:00:02:08:00 SRC=172.17.0.2 DST=10.10.126.101 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=31888 DF PROTO=TCP SPT=54408 DPT=12580 WINDOW=29200 RES=0x00 SYN URGP=0
```
做DNAT后，再次路由到docker0，于是走forward链，但是没有匹配上nat postrouting，也就没有做SNAT：
```shell?linenums
Jan 18 13:51:10 pc-baim kernel: [279750.806581] [TonyBai]-FwdId0Od0:IN=docker0 OUT=docker0 PHYSIN=veth44a97d7 PHYSOUT=veth0d66af2 MAC=02:42:ac:11:00:04:02:42:ac:11:00:02:08:00 SRC=172.17.0.2 DST=172.17.0.4 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=31888 DF PROTO=TCP SPT=54408 DPT=12580 WINDOW=29200 RES=0x00 SYN URGP=0
Jan 18 13:51:10 pc-baim kernel: [279750.806608] [TonyBai]-NatPostrouteEnterIN= OUT=docker0 PHYSIN=veth44a97d7 PHYSOUT=veth0d66af2 SRC=172.17.0.2 DST=172.17.0.4 LEN=60 TOS=0x10 PREC=0x00 TTL=64 ID=31888 DF PROTO=TCP SPT=54408 DPT=12580 WINDOW=29200 RES=0x00 SYN URGP=0
```
container3回复ack sync。宿主机从docker0收到ack sync包，目的地址172.17.0.2，再次路由到docker0。
```shell?linenums
Jan 18 13:51:10 pc-baim kernel: [279750.806719] [TonyBai]-RawPrerouting:IN=docker0 OUT= PHYSIN=veth0d66af2 MAC=02:42:ac:11:00:02:02:42:ac:11:00:04:08:00 SRC=172.17.0.4 DST=172.17.0.2 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=12580 DPT=54408 WINDOW=28960 RES=0x00 ACK SYN URGP=0
Jan 18 13:51:10 pc-baim kernel: [279750.806746] [TonyBai]-FwdOd0:IN=docker0 OUT=docker0 PHYSIN=veth0d66af2 PHYSOUT=veth44a97d7 MAC=02:42:ac:11:00:02:02:42:ac:11:00:04:08:00 SRC=172.17.0.4 DST=172.17.0.2 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=12580 DPT=54408 WINDOW=28960 RES=0x00 ACK SYN URGP=0
```
由于之前docker0上做过DNAT，因此从docker0回到172.17.0.2时，src地址会自动un-DNAT，从172.17.0.4改为10.10.126.101，不过在上面日志中看不出这一点。

172.17.0.2回复ack，握手完成，DNAT自动进行：
```shell?linenums
Jan 18 13:51:10 pc-baim kernel: [279750.806823] [TonyBai]-RawPrerouting:IN=docker0 OUT= PHYSIN=veth44a97d7 MAC=02:42:23:39:fd:f5:02:42:ac:11:00:02:08:00 SRC=172.17.0.2 DST=10.10.126.101 LEN=52 TOS=0x10 PREC=0x00 TTL=64 ID=31889 DF PROTO=TCP SPT=54408 DPT=12580 WINDOW=229 RES=0x00 ACK URGP=0
Jan 18 13:51:10 pc-baim kernel: [279750.806852] [TonyBai]-FwdOd0:IN=docker0 OUT=docker0 PHYSIN=veth44a97d7 PHYSOUT=veth0d66af2 MAC=02:42:ac:11:00:04:02:42:ac:11:00:02:08:00 SRC=172.17.0.2 DST=172.17.0.4 LEN=52 TOS=0x10 PREC=0x00 TTL=64 ID=31889 DF PROTO=TCP SPT=54408 DPT=12580 WINDOW=229 RES=0x00 ACK URGP=0
```
三、网络性能考量
docker-proxy常被docker使用者诟病，一是因为每个映射端口都要启动一个docker-proxy进程，映射端口多了，大量进程被创建、被调度势必消耗大量系统资源；二来，在高负载场合，docker-proxy的转发性能也力不从心。理论上，docker-proxy代理转发流量的方式在性能方面要比单纯iptables DNAT要弱上一些。不过我在单机上通过 sparkyfish 测试的结果倒是二者相差不大，估计是因为我仅仅启动了一个docker-proxy，系统负荷并不大的缘故。


## 1.3 Docker容器的DNS名和主机名不要乱修改
![enter description here][17]
![enter description here][18]
![enter description here][19]
![enter description here][20]

## 1.4 Docker网络实现原理分析
### Docker daemon网络配置
![enter description here][21]
![enter description here][22]
![enter description here][23]
![enter description here][24]
![enter description here][25]
![enter description here][26]

### libcontainer网络配置原理（docker run容器配置）
![enter description here][27]
![enter description here][28]
![enter description here][29]
![enter description here][30]
![enter description here][31]
![enter description here][32]
![enter description here][33]
![enter description here][34]
![enter description here][35]
![enter description here][36]
![enter description here][37]
![enter description here][38]


## 1.5 同宿主机容器互联（link）
容器间通信由Docker daemon的启动参数-icc控制。-icc为true，保证默认可以互联。但为了保证容器以及主机的安全，-icc通常设置为false。这种情况下该如何解决容器间的通信呢？最长用的方式是端口映射，这种方式不够安全，而且需要经过NAT，效率也不高。这时候就需要Docker连接（linking）。Docker的连接系统可以在两个容器之间建立一个安全的通道，使得接受容器（如Web容器）可以通过通道得到容器（如数据库服务）指定的相关信息。
在Docker1.9版本后，网络操作独立出一个命令组（docker network），link方式也与原来不同了。Docker为了向上兼容，若容器使用默认的bridge模式网络，则会默认使用传统的link系统；而使用用户自定义的网络，则会使用新的link系统。
### 传统link原理
![enter description here][39]
![enter description here][40]
![enter description here][41]
![enter description here][42]
![enter description here][43]
![enter description here][44]

### 新的link介绍
![enter description here][45]
![enter description here][46]
![enter description here][47]
![enter description here][48]

关于network的操作，看文章《Docker 1.9的新网络特性，以及Overlay详解 - 推酷》。
处于同一个network的容器间能互通，不通network的容器不能Ping通。看【1.1 Docker架构实现】

## 1.6 使用TC工具限制Docker容器流量
Docker已经为容器的资源限制做了很多工具，但是在网络带宽方面却没有进行限制，这就可能导致一些安全隐患，尤其是使用Docker构建容器云时，可能存在多租户共用宿主机资源的情况，这些问题就显得尤为突出，极有可能出现诸如容器内的Dos攻击等危害。无限制的大流量访问会破坏容器的实时交互能力，所以需要对容器流量进行限制。
Traffic Controller（TC）是Linux的流量控制模块，其原理是为数据包建立队列，并且定义了队列中数据包的发送规则，从而实现在技术上对流量进行限制、调度等控制操作。
Traffic Controller（TC）中的流量控制队列分为两种：无类队列和分类队列。
* 无类队列    就是对进入网卡的数据进行统一对待，无类队列能够接受数据包并对网卡流量整形，但是不能对数据包进行细致划分，无类队列规定主要有PFIFO_FAST、TBF和SFQ等，无类队列的流量整形手段主要是排序、限速以及丢包。
* 分类队列    是对进入网卡的数据包根据不同的需求以分类的方式区分对待。数据包进入分类队列后，通过过滤器对数据包进行分类，过滤器返回一个决定，这个决定指向某一个分类，队列就根据这个返回的决定把数据包发送到相应的某一类队列中排队。每个子类可以再次使用自己的过滤器对数据包进一步的分类，直到不需要分类为止，数据包最终会进入相关类的队列中排队。
* 
Traffic Controller（TC）流量控制方式分为4种：
* SHAPING：流量被限制时，它的传输速率被控制在某个值一下，限制阈值可以远小于有效带宽，这样可以平滑网络的突发流量，使网络更稳定，SHAPING方式适用于限制外出的流量。
* SCHEDULING：通过调度数据包传输的优先级数据，可以在带宽范围内对不同的传输流按照优先级分配，适用于限制外出的流量。
* POLICING：SHAPING用于处理向外流量，而POLICING用于处理接收到数据，对数据流量进行限制。
* DROPPING：如果流量超过设置的阈值就丢弃数据包，向内向外皆有效。
下面将分别介绍无类队列和分类队列的使用方法：
* 无类队列的使用
    无类队列的使用方法非常简单，TBF（Token Bucket Filter）是无类队列中比较常用的一种队列，TBF只是对数据包流量进行SHAPING，并不做SCHEDULING。如果只是简单地限制网卡的流量，这会是一个很高效的方式。使用无类队列进行流量限制比较简单，Linux下对流量进行限制的命令工具是tc，命令如下所示：
```shell?linenums
tc disc add dev eth1 root handle 1:0 tbf rate 128kbit burst 1000 latency 50ms
```
其中各字段含义如下：
* tc dic add dev eth1    表示在设备eth1上添加队列
* root    表示根节点，没有父亲节点；
* handle 1:0    表示队列句柄；
* tbf    表示使用无类队列TBF；
* rate 128kbit    速率是128kbit；
* burst 1000    桶尺寸为1000；
* latency 50ms    数据包最多等待50ms。

* 分类队列的使用
    如果需要对数据包进一步细分，对不同类型数据进行区别对待，分类队列就非常适合。CBQ（Class Based Queue）是一种比较常用的分类队列。在分类器中多了类和过滤器两个概念。通过过滤器把数据包划分到不同的类里面，再递归地处理这些类。下面以一个简单例子来描述分类队列的使用。
    假设我们有一个如下的场景：主机上有一张带宽为100Mbit/s的物理网卡，在这个主机上开启了3个服务：FTP服务、snmp服务以及http服务，我们需要对这3种服务的带宽进行限制，那么可以进行如下操作。
首先建立一个根队列：
```shell?linenums
tc disc add dev eth0 root handle 1:0 cbq bandwidth 100Mbit avpkt 1000 cell 8
```
然后在此队列下建立3个类：
```shell?linenums
tc class add dev eth0 parent 1:0 classic 1:1 cbq bandwidth 100Mbit rate 5Mbit weight 0.5Mbit prio 5 cell 8 avpkt 1000
tc class add dev eth0 parent 1:0 classic 1:2 cbq bandwidth 100Mbit rate 10Mbit weight 0.5Mbit prio 5 cell 8 avpkt 1000
tc class add dev eth0 parent 1:0 classic 1:3 cbq bandwidth 100Mbit rate 15Mbit weight 0.5Mbit prio 5 cell 8 avpkt 1000
```
接下来再在3个类下建立队列或者对类进一步划分：
```shell?linenums
tc qdisc add dev eth0 parent 1:1 handle 10:0
tc qdisc add dev eth0 parent 1:2 handle 20:0
tc qdisc add dev eth0 parent 1:3 handle 30:0
```
最后再为根队列建立3个过滤器：
```shell?linenums
tc filter add dev eth0 parent 1:0 protocol ip prio 1 u32 math ip sport 20 0xfffff flowid 1:1
tc filter add dev eth0 parent 1:0 protocol ip prio 1 u32 math ip sport 161 0xfffff flowid 1:2
tc filter add dev eth0 parent 1:0 protocol ip prio 1 u32 math ip sport 80 0xfffff flowid 1:3
```
由此可见，我们为根队列建立了3个分类，分别对ftp、snmp以及http这3种服务的数据包进行限制，其余数据包将不受限制。

tc对veth限额实现容器的网络限额控制：
1.  找到容器的vethname
```shell?linenums
#!/bin/bash  
#filename:getveth.sh  
#author:wade  
container_name=$1  
if [ -z $1 ] ; then  
    echo "Usage: ./getveth.sh container_name"  
    exit 1  
fi  
  
if [ `docker inspect -f "{{.State.Pid}}" ${container_name} &>>/dev/null && echo 0 || echo 1` -eq 1 ];then  
echo "no this container:${container_name}"  
exit 1  
fi  
pid=`docker inspect -f "{{.State.Pid}}" ${container_name}`  
mkdir -p /var/run/netns  
ln -sf /proc/$pid/ns/net "/var/run/netns/${container_name}"  
index=`ip netns exec "${container_name}" ip link show eth0 | head -n1 | sed s/:.*//`  
let index=index+1  
vethname=`ip link show | grep "^${index}:" | sed "s/${index}: .*:.*/\1/"`  
echo $vethname  
rm -f "/var/run/netns/${container_name}"
```
2. docker容器下载流量限额
在node节点上用tc设置对应veth的流量带宽
```shell?linenums
tc qdisc add dev vethname root  tbf rate 10mbit latency 50ms burst 10000 mpu 64 mtu 15000 
```
3. docker容器上传流量限额
在容器内使用tc设置eth0设备的流量带宽
```shell?linenums
tc qdisc add dev eth0 root  tbf rate 10mbit latency 50ms burst 10000 mpu 64 mtu 15000
```

# 2. 跨主机集群容器网络
前面我们详细解读了Docker中libnetwork提供的4种驱动，它们各有千秋，但实际上每一种方式都有一定的局限性。假设需要运营一个数据中心的网络，我们有许多宿主机，每台宿主机上运行了数百个甚至上千个Docker容器，不可避免遭遇ocker容器跨主机通信的问题。目前Docker默认网络环境下，单台主机上的Docker容器可以通过docker0网桥直接通信，而不同主机上的Docker容器之间通过在主机上做端口映射进行通信。这种端口映射方式对很多集群应用来说极不方便，而且还损耗性能。使用4种网络驱动的具体情况如下：

 - 使用host确定可以让容器与宿主机公用一个网络栈，这么做看似解决了网络问题，可实际上并未使用network namespace的隔离，缺乏安全性。
 - 使用Docker默认的bridge驱动，容器没有对外IP，只能通过NAT来实现对外通信。这种方式不能解决跨主机容器间直接通信的问题。
 - 使用overlay驱动，可以用于支持跨主机的网络通信，但必须要配合Swarm进行配置和使用才能实现跨主机的网络通信。在Docker 1.9版本之前，社区中就已经有许多第三方的工具或方法尝试解决这个问题，例如Macvlan、Pipework、Flannel、Weave等。虽然这些方案在实现细节上存在很多差异，但其思路无非分为两种：二层VLAN网络和Overlay网络。后面详讲。看文章了解Docker1.9后的overlay驱动和swarm使用文章《Docker 1.9的新网络特性，以及Overlay详解》
 - 使用null驱动，实际上不进行任何网络设置。

实现数据中心大量容器间的跨主机网络通信，容器间网络的共享与隔离，为了管理成千上万个容器时可以更加自动化地进行网络配置，我们需要学习更高级的网络实践方案。

下面听不懂没关系：
首先简单介绍下现有的容器网络方案，网上也看了好多对比，大家之前都是基于实现方式来分，

 - 隧道方案
通过隧道，或者说Overlay Networking的方式：
**Weave**，UDP广播，本机建立新的BR，通过PCAP互通。
**Open vSwitch（OVS）**，基于VxLAN和GRE协议，但是性能方面损失比较严重。
**Flannel**，UDP广播，VxLan。

隧道方案在IaaS层的网络中应用也比较多，大家共识是随着节点规模的增长复杂度会提升，而且出了网络问题跟踪起来比较麻烦，大规模集群情况下这是需要考虑的一个点。

- 路由方案
还有另外一类方式是通过路由来实现，比较典型的代表有：
**Calico**，基于BGP协议的路由方案，支持很细致的ACL控制，对混合云亲和度比较高。
**Macvlan**，从逻辑和Kernel层来看隔离性和性能最优的方案，基于二层隔离，所以需要二层路由器支持，大多数云服务商不支持，所以混合云上比较难以实现。

路由方案一般是从3层或者2层实现隔离和跨主机容器互通的，出了问题也很容易排查。

我觉得Docker 1.9以后再讨论容器网络方案（因为Docker增加了Overlay驱动插件），不仅要看实现方式，而且还要看网络模型的“站队”，比如说你到底是要用Docker原生的 “CNM”，还是CoreOS，谷歌主推的“CNI”。
Docker Libnetwork Container Network Model（CNM）阵营

Docker Swarm overlay
Macvlan & IP network drivers
Calico
Contiv（from Cisco）

Docker Libnetwork的优势就是原生，而且和Docker容器生命周期结合紧密；缺点也可以理解为是原生，被Docker“绑架”。
Container Network Interface（CNI）阵营

Kubernetes
Weave
Macvlan
Flannel
Calico
Contiv
Mesos CNI

CNI的优势是兼容其他容器技术（e.g. rkt）及上层编排系统（Kuberneres & Mesos)，而且社区活跃势头迅猛，Kubernetes加上CoreOS主推；缺点是非Docker原生。

## 2.1 利用虚拟网桥将Docker容器桥接到本地网络
### 2.1.1 将Docker容器与宿主机配置在本地的网络环境中

如果想要使Docker容器和容器主机处于同一个网络，那么容器和主机应该处在一个二层网络中。能想到的场景就是把两台机器连在同一个交换机上，或者连在不同的级联交换机上。在虚拟场景下，虚拟网桥可以将容器连在一个二层网络中，只要将主机的网卡桥接到虚拟网桥中，就能将容器和主机的网络连接起来。构建完拓扑结构后，只需再给Docker容器分配一个本地局域网IP就OK了。
下面通过一个例子来分析这个过程。本地网络为`10.10.103.0/24`，网关为`10.10.103.254`，有一台IP地址为`10.10.103.91/24`的主机（网卡为eth0），要在这台主机上启动一个名为test1的Docker容器，并给它配置IP为`10.10.103.95/24`。由于并不需要Docker提供的网络，所以用`--net=none`参数来启动容器。具体示例如下：
![enter description here][49]
![enter description here][50]
配置完后，Docker容器和主机连接的网络拓扑如图所示：
![enter description here][51]
我的操作：
本地网络为`10.0.2.0/24`，网关为`10.0.2.255`，宿主机IP地址为`10.0.2.15/24`（网卡为eth0），test1容器，配置IP为`10.0.2.16/24`。
```shell?linenums
[root@localhost ~]# ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 52:54:00:88:15:b6 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 85824sec preferred_lft 85824sec
    inet6 fe80::5054:ff:fe88:15b6/64 scope link
       valid_lft forever preferred_lft forever
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN
    link/ether 02:42:13:40:9f:77 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 scope global docker0
       valid_lft forever preferred_lft forever
[root@localhost ~]#
[root@localhost ~]#
[root@localhost ~]# docker run -itd --name test1 --net=none centos /bin/bash
Unable to find image 'centos:latest' locally
Trying to pull repository docker.io/library/centos ...
latest: Pulling from docker.io/library/centos
d5e46245fe40: Pull complete
Digest: sha256:aebf12af704307dfa0079b3babdca8d7e8ff6564696882bcb5d11f1d461f9ee9
3474ee9a3fde4f473c38af597a7f355be6b3c2148c5a4766ca6ce55c89954122
[root@localhost ~]#
[root@localhost ~]#创建供容器使用的网桥br0
[root@localhost ~]# brctl addbr br0
[root@localhost ~]# ip link set br0 up
[root@localhost ~]#将主机eth0桥接到br0上，并把eth0的IP配置在br0上。由于是远程操作，会导致网络断开，这里放在一行。
[root@localhost ~]# ip addr add 10.0.2.15/24 dev br0; \
	ip addr del 10.0.2.16/24 dev eth0; \
	brctl addif br0 eth0; \
	ip route del default; \
	ip route add default via 10.0.2.255 dev br0
RTNETLINK answers: Cannot assign requested address
RTNETLINK answers: Network is unreachable
[root@localhost ~]#
[root@localhost ~]#
[root@localhost ~]#找到test1的PID，保存到pid中
[root@localhost ~]# pid=$(docker inspect --format '{{.State.Pid}}' test1)
[root@localhost ~]# 将容器的network namespace添加到/var/run/netns目录下
[root@localhost ~]# mkdir -p /var/run/netns
[root@localhost ~]# ln -s /proc/$pid/ns/net /var/run/netns/$pid
[root@localhost ~]#
[root@localhost ~]#创建用于连接网桥和Docker容器的网卡设备，将veth-a连接到br0网桥上，veth-b放入容器的网络命名空间里
[root@localhost ~]# ip link add veth-a type veth peer name veth-b
[root@localhost ~]# brctl addif br0 veth-a
[root@localhost ~]# ip link set veth-a up
[root@localhost ~]# 将veth-b放到test1的network namespace中，重命名为eth0，并为其配置IP和默认路由
[root@localhost ~]# ip link set veth-b netns $pid
[root@localhost ~]# ip netns exec $pid ip link set dev veth-b name eth0
[root@localhost ~]# ip netns exec $pid ip link set eth0 up
[root@localhost ~]# ip netns exec $pid ip addr add 10.0.2.16/24 dev eth0
[root@localhost ~]# ip netns exec $pid ip route add default via 10.0.2.255
RTNETLINK answers: Network is unreachable
[root@localhost ~]#
[root@localhost ~]#
[root@localhost ~]#在容器内能拼通容器外宿主机
[root@7b7799616fa8 /]# ping 10.0.2.15
PING 10.0.2.15 (10.0.2.15) 56(84) bytes of data.
64 bytes from 10.0.2.15: icmp_seq=1 ttl=64 time=0.199 ms
64 bytes from 10.0.2.15: icmp_seq=2 ttl=64 time=0.063 ms
64 bytes from 10.0.2.15: icmp_seq=3 ttl=64 time=0.059 ms
^C
--- 10.0.2.15 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2001ms
rtt min/avg/max/mdev = 0.059/0.107/0.199/0.065 ms
[root@7b7799616fa8 /]# [root@localhost ~]#
[root@localhost ~]# ping 10.0.2.16
PING 10.0.2.16 (10.0.2.16) 56(84) bytes of data.
64 bytes from 10.0.2.16: icmp_seq=1 ttl=64 time=0.039 ms
64 bytes from 10.0.2.16: icmp_seq=2 ttl=64 time=0.069 ms
64 bytes from 10.0.2.16: icmp_seq=3 ttl=64 time=0.056 ms
^C
--- 10.0.2.16 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 1999ms
rtt min/avg/max/mdev = 0.039/0.054/0.069/0.014 ms
[root@localhost ~]#
但在其他主机上拼不通这个IP
➜  ~ ping 10.0.2.16
PING 10.0.2.16 (10.0.2.16): 56 data bytes
Request timeout for icmp_seq 0
Request timeout for icmp_seq 1
Request timeout for icmp_seq 2
Request timeout for icmp_seq 3
Request timeout for icmp_seq 4
Request timeout for icmp_seq 5
Request timeout for icmp_seq 6
^C
--- 10.0.2.16 ping statistics ---
8 packets transmitted, 0 packets received, 100.0% packet loss
➜  ~
➜  ~
```

### 2.1.2 使用pipework工具配置各种类型网络
从上面看到，配置Docker容器的网络是相当繁琐的，如果需要经常自定义Docker网络，可提炼上述过程，编写成shell脚本。事实上，目前已有这样的工具帮助用户扩展Docker的网络功能。就如Docker公司工程师研发的pipework工具，号称是容器的SDN解决方案，可以在复杂场景下将容器连接起来。随着Dockr的网络不断改进，pipework工具的很多功能可能会被Docker原生支持。

#### 1 将Docker容器与宿主机配置在本地的网络环境中
在上面，我们使用了很多ip命令来配置test1容器的IP地址和网关，用pipework工具则可以很方便地完成配置，具体过程如下：
```shell?linenums
#下载pipework
$ git clone https://github.com/jpetazzo/pipework
$ 将pipework脚本放入PATH环境变量所指定的目录下，如/usr/local/bin
$ cp ~/pipework/pipework /usr/local/bin/
$
# 完成test1的配置
$ pipework br0 test1 10.10.103.95/24@10.10.103.254
```
这一行配置命令执行的操作如下：
- 查看主机是否存在br0网桥，不存在就创建；
- 向容器test1中加入一块名为eth1的网卡，并设置IP地址为10.10.103.95/24；
- 若容器test1中已经有默认路由，则删掉，把10.10.103.254设为默认路由的网关；
- 将容器test1连接到之前创建的网桥br0上。

这个过程和之前采用ip命令配置的过程类似，查看pipework脚本实现的这部分内容和我们的差不多。

#### 2 支持使用macvlan设备将容器连接到本地网络
除了使用Linux Bridge将Docker容器桥接到本地网络之外，还有另外一种方式，即使用主机网卡的macvlan子设备。macvlan设备是从网卡上虚拟出一块新网卡，它和主网卡分别有不同的MAC地址，可以配置独立的IP地址。目前Docker网络本身不提供macvlan支持，但可以借助pipework来完成macvlan配置。如果采用macvlan来完成之前例子，那么整个过程只需要执行一条命令：
```shell?linenums
$ pipework eth0 test1 10.10.103.95/24@10.10.103.254
```
这里，pipework的参数1 eth0是主机上的一块以太网卡，而非网桥。pipework采用macvlan设备作为test1容器的网卡，不会再创建veth pair设备来连接容器和网桥，操作过程如下：
- 从主机的eth0上创建一块macvlan设备，将macvlan设备放入到test1容器中并命名为eth1；
- 为容器test1中新添加的网卡配置IP地址为10.10.103.95/24；
- 若容器test1中已经有默认路由，则删掉，把10.10.103.254设为默认路由的网关。
除了创建的设备不一样外，给容器配置IP和网关的代码还是和上文一致。
pipework创建macvlan设备的代码如下：
![enter description here][52]
从宿主机eth0创建出的macvlan设备放在容器test1后，容器test1就可以和本地网络中的其他宿主机通信了。
但是，如果在容器test1所在的宿主机上却访问不了容器test1内，因为进出macvlan设备的流量被主网卡eth0隔离了，宿主机不能通过eth0网卡访问macvlan设备，即这里的容器test1。要解决这个问题，需要在宿主机eth0上再创建一个macvlan设备，将宿主机eth0的IP地址移动这个macvlan设备上，代码如下（若远程操作，请放在一条命令中执行）。
```shell?linenums
ip addr del 10.10.103.91/24 dev eth0
# 用ip命令在宿主机eth0上创建maclan设备eth0m
ip link add link eth0 dev eth0m type macvlan mode bridge
# 启eth0m设备
ip link set eth0m up
# 设置eth0m设备的IP地址
ip addr add 10.10.103.91/24 dev eth0m
# 把10.10.103.254设为默认路由的网关
route add default gw 10.10.103.254
```

#### 3 通过外部DHCP服务器获取并设置容器的IP
如果Docker要介入的网络环境中存在DHCP服务器，那么Docker容器就可以通过发送DHCP请求获取新网卡的网络配置信息。具体用法是将pipework指令中的IP地址参数替换为dhcp，示例如下：
```shell?linenums
#配置直接将宿主机eth0的macvlan的子设备作为容器test1的网卡，为新网卡设置IP，设置默认路由的网关
pipework eth0 test1 10.10.103.95/24@10.10.103.254
通过主机网络中的DHCP服务器获取IP地址的命令
pipework eth0 test1 dhcp
```
![enter description here][53]
从上面代码看出，DHCP服务除了要求主机环境中存在DHCP服务器外，Docker主机上还必须安装有DHCP客户端（如udhcpc、dhclient或者dhcpcd）。pipework根据不同的DHCP客户端执行不同的命令发送DHCP请求。自己再深入研究。

## 2.2 桥接
上面3节都演示了如何使用虚拟网桥将Docker容器桥接到本地网络环境中，按照这种方法，把同一局域网中不同主机上的Docker容器都配置在主机网络环境中，它们之间可以直接通信。但这么做可能会出现下列问题：
- Docker容器占用主机网络的IP地址；
- 大量Docker容器可能引起广播风暴，导致主机所在网络性能的下降；
- Docker容器连在主机网络中可能引起安全问题。
因此，如果情况不是无法回避，必须将Docker容器连接在主机网络中，最好还是将其分离开。为了隔离Docker容器间网络和主机网络，需要额外使用一块网卡桥接Docker容器。思路还是与采用一块网卡时一样：在所有主机上用虚拟网桥（如docker0网桥）将本机的Docker容器连接起来，然后将一块网卡（eth1）加入到虚拟网桥中，使所有主机上的虚拟网桥级联在一起，这样，不同主机上的Docker容器也就如同连在了一个大的逻辑交换机上。这就是桥接。
关于Docker容器的IP，由于不同机器上的Docker容器可能获得相同的IP地址，因此需要解决IP冲突，我们可以使用pipework为么一个容器分配一个不同的IP，那么多机器，每个上的容器都重新设置,这种方法太烦琐了，采用另一种办法——为每一台主机上的Docker daemon指定不同的--fixed-cidr参数，将不同主机上的Docker容器的地址限定在不同的网段中。
![enter description here][54]
如图，两台Ubuntu的主机host1和host2，每台主机上各有两块网卡eth0和eth1。eth0作为主机的主网卡连在主机的局域网环境中，其IP分别为10.10.103.91/24和10.10.103.92/24；eth1用来桥接不同主机上的Docker容器，eth1不需要配置IP。
Docker安装完成后，在host1主机上看到docker0的IP为172.17.42.1/16，Docker容器也就是从docker0所在的网络中获取IP。在本例中，将host1上的Docker容器的IP范围限制在172.17.1.0/24网段中，将host2上的Docker容器的IP范围限制在172.17.2.0/24网段中，同时将host2的docker0网桥地址改为172.17.42.2/16，以避免和host1的docker0的IP冲突。然后将eth1桥接到docker0中，配置如下：
```shell?linenums
#在host1上作如下操作
$ echo 'DOCKER_OPTS="--fixed-cidr=172.17.1.1/24"' >> /etc/default/docker
$ service docker stop
$ service docker start
#将eth1网卡接入到docker0网桥中
$ brctl addif docker0 eth1

#在host2上作如下操作
$ echo 'DOCKER_OPTS="--fixed-cidr=172.17.2.1/24"' >> /etc/default/docker
#为避免和host1的docker0的IP冲突，修改docker0的IP
$ ifconfig docker0 172.17.42.2/16
$ service docker stop
$ service docker start
#将eth1网卡接入到docker0网桥中
$ brctl addif docker0 eth1
```
对docker0网桥设置IP，子网掩码的这一步配置只是暂时生效，再重启机器后，配置会失效。如果需要持久化配置，可将docker0配置信息写入/etc/network/interfaces目录下，示例如下：
```shell?linenums
auto docker0
iface docker0 inet static
	address 172.17.42.2
	netmask 255.255.0.0
	bridge_ports eth1
	bridge_stp off
	bridge_fd 0
```
在host1和host2上分别创建两个容器con1、con2，使用nc命令测试con1和con2的连接，示例如下：
![enter description here][55]
![enter description here][56]
从上里可以发现，容器con1和con2可以成功通信了。

容器con1（172.17.1.1）向容器con2（172.17.2.1）发送数据的过程是这样的：首先，通过查看本身的路由表发现目的地址和自己处于同一个网段，那么就不需要将数据发往网关，可以直接发给con2，con1通过ARP广播获取到con2的MAC地址；然后，构建以太网帧发往con2即可。此过程数据流经的路径如上图中两个容器的eth0网卡所连接的路径，其中docker0网桥充当普通的交换机转发数据帧。

## 2.3 直接路由
桥接方式是将所有主机上的Docker容器放在一个二层网络中，它们之间通信是由交换机直接转发，不通过路由器。另一种跨主机通信的方式是通过在主机中添加静态路由实现的。如果有两台主机host1和host2，两主机上的Docker容器是两个独立的二层网络，将con1发往con2的数据流先转发到主机host2上，再由host2再转发到其上的Docker容器中；反之亦然。
由于使用容器的IP进行路由，就需要避免不同主机上的Docker容器使用相同的IP， 所以应该为不同的主机分配不同的IP子网。下面演示场景。
图中，两台Ubuntu的主机host1和host2，每台主机上有一块往卡。host1的IP地址为10.10.103.91/24，host2的IP地址为10.10.103.92/24，。host1上的Docker容器在172.17.1.0/24子网中，host2的Docker容器在172.17.2.0/24子网中，并且在两台主机上有这样的规则——所有目的地址为172.17.1.0/24的包都被转发到host1，目的地址为172.17.2.0/24的包都被转发到host2.为此做如下配置。
![enter description here][57]
```shell?linenums
#在host1上做如下操作
#为防止与不同主机的IP冲突，影响配置路由规则，配置docker0的IP地址
$ ifconfig docker0 172.17.1.254/24
$ service docker restart
#添加路由，将目的地址为172.17.2.0/24的包转发到host2
$ route add -net 172.17.2.0 netmask 255.255.255.0 gw 10.10.103.92
#配置iptables规则
$ iptables -t nat -F POSTROUTING
$ iptables -t nat -A POSTROUTING -s 172.17.1.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
#启动容器con1
$ docker run -it --name con1 ubuntu /bin/bash
#在con1容器中
#nc -l 9000


#在host2上做如下操作
#为防止与不同主机的IP冲突，影响配置路由规则，配置docker0的IP地址
$ ifconfig docker0 172.17.2.254/24
$ service docker restart
#添加路由，将目的地址为172.17.1.0/24的包转发到host1
$ route add -net 172.17.1.0 netmask 255.255.255.0 gw 10.10.103.91
#配置iptables规则
$ iptables -t nat -F POSTROUTING
$ iptables -t nat -A POSTROUTING -s 172.17.2.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
#启动容器con2
$ docker run -it --name con2 ubuntu /bin/bash
#在con2容器中
#nc -w 1 -v 172.17.1.1 9000
Connection to 172.17.1.1 9000 port [tcp/*] succeeded!
```
> 需要注意的是，此处配置容器IP范围的方法和之前桥接网络中所使用的方法不同。在桥接网络中，所有主机上的容器都在172.17.0.0/16这个大网络中，这从docker0的IP（172.17.42.1/16）可以看出，只是使用`--fixed-cidr`参数将不同主机的容器限制在这个IP网段的亿个小范围内。而在直接路由方法中，不同主机上的Docker容器不在同一个网络中，它们有不同的网络号，如果将host1上的docker0的IP设为172.17.1.254/24，那么host1上的Docker容器就只能从172.17.1.0/24网段中获取IP。所以，尽管这两种方法都是用了相同的IP地址范围，但它们的网络号是不同的，因此涉及的转发机制也不相同，桥接网络是二层通信，通过MAC地址转发；直接路由为三层通信，通过IP地址进行路由转发。

上面配置的iptables规则是什么作用呢？其中在，启动Docker daemon时会创建如下iptables规则，和这个类似：
```shell?linenums
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
```
直接解释这里，
```shell?linenums
-A POSTROUTING -s 172.17.1.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
```
从con1发往con2的包，在主机eth0转发出去时，这条规则会将包的源地址改为eth0地址（10.10.103.91），因此con2看到包时从10.10.103.91上发过来的。反过来从con2发往con1的包也是相同的原理。尽管这并不影响它们之间的通信，但两个容器并没有真正“看到”对方。
```shell?linenums
iptables -t nat -F POSTROUTING
```
这条iptables规则表示清空nat表。
如果将上面转发规则删除，这样两容器之间的通信就没有SNAT转换了，会导致容器访问不了外部网络。有这条MASQUERADE规则到POSTROUTING链，使所有目标地址不是172.17.0.0/16的包都经过SNAT转换。
**综上所述**，从con1发往con2的包，首先发往con1的网关docker0（172.17.1.254），然后通过查看主机的路由得知需要将包发给host2（10.10.103.92），包到达host2后再转发给host2的docker0（172.17.2.254），最后到达容器con2中。

桥接方法和直接路由方法可以实现跨主机通信，但要求主机是在同一个局域网中。如果两台主机在不同的二层网络中，又该如何实现容器间的跨主机通信呢？下面OVS 隧道会专门处理这个场景。

当集群中机器的数量很多，假设有100台服务器，那么就需要在每台服务器上手动添加到另外99台服务器docker0的路由规则。为了减少手动操作，可以使用Quagga软件来实现路由规则的动态添加。
### 2.4.1 使用Quagga软件实现路由规则的动态添加
使用Quagga软件有两种办法：可以在每台服务器上安装Quagga软件并启动，还可以下载Quagga容器来运行。
*下面例子只是用来看，在下面有具体的例子*
本例使用index.alauda.cn/georce镜像启动Quagga。在每台Node上下载该Docker镜像：
```shell?linenums
$ docker pull index.alauda.cn/georce/router
```
再运行Quagga路由器之前，需要确保每个Node上docker0王巧的子网地址不能重叠，也不能与物理机所在的网络重叠，这需要提前规划好。

下面以3个Node为例，使用ifconfig命令修改docker0网桥的地址和子网（假设Node所在的物理网络不是10.1.X.X地址段）：
```shell?linenums
Node 1: # ifconfig docker0 10.1.10.1/24
Node 2: # ifconfig docker0 10.1.20.1/24
Node 3: # ifconfig docker0 10.1.30.1/24
```
然后在每个Node上启动Quagga容器。需要说明的是，Quagga需要以`--privileged`特权模式运行，并且指定`--net=host`，表示直接使用物理机的网络：
```shell?linenums
$ docker run -itd --name=router --privileged --net=host index.alauda.cn/georce/router
```
启动完成后，**Quagga会互相学习来完成到其他机器的docker0路由规则的添加**。

一段时间后，在Node1上使用route -n命令来查看路由表，可以看到Quagga自动添加了两条到Node2和到Node3上docker0的路由规则。
```shell?linenums
# route -n
Kernel IP routing table
Destination   Gateway         Genmask         Flags   Metric  Ref   Use   Iface
0.0.0.0       192.168.1.128   0.0.0.0         UG      0       0     0     eth0
10.1.10.0     0.0.0.0         255.255.255.0   U       0       0     0     docker0
10.1.20.0     192.168.1.129   255.255.255.0   UG      20      0     0     eth0
10.1.30.0     192.168.1.130   255.255.255.0   UG      20      0     0     eth0
```
在Node2上查看路由表，可以看到自动添加了两条到Node1和Node3上docker0的路由规则。
```shell?linenums
# route -n
Kernel IP routing table
Destination   Gateway         Genmask         Flags   Metric  Ref   Use   Iface
0.0.0.0       192.168.1.129   0.0.0.0         UG      0       0     0     eth0
10.1.20.0     0.0.0.0         255.255.255.0   U       0       0     0     docker0
10.1.10.0     192.168.1.128   255.255.255.0   UG      20      0     0     eth0
10.1.30.0     192.168.1.130   255.255.255.0   UG      20      0     0     eth0
```
至此，所有Node上的docker0都可以互相互通了。

看文章《【技巧】使用Quagga软件的RIPV2自动学习功能实现路由规则的动态添加，配置直接路由跨主机网络 - DockOne.io》

## 2.4 OVS划分VLAN（使用隧道方案Open vSwitch代替docker0实现Docker容器的VLAN划分（会用到pipework脚本对Open vSwitch工具命令的一些封装））
在计算机网络中，传统的交换机虽然能隔离冲突域，提高每一个端口的性能，但并不能隔离广播域，当网络中的机器足够多时会引发广播风暴，导致主机所在网络性能的下降。同时，不同部门、不同组织的机器连在同一个二层网络中也会造成安全问题。因此，在交换机中划分子网、隔离广播域的思路便形成了VLAN的概念。VLAN（Virtual Local Area Network）即虚拟局域网，按照功能、部门等因素将网络中的机器进行划分，使之分属于不同的部分，每一个部分形成一个虚拟的局域网络，共享一个单独的广播域。这样就可以把一个大型交换网络划分为许多个独立的广播域，即VLAN。
VLAN技术将一个二层网络的机器隔离开来，那么如何区分不同VLAN的流量呢？IEEE802.1q协议规定了VLAN的实现方法，在传统的以太网帧（传统的以太网是没有VLAN tag字段的）中再添加一个VLAN tag字段，用于标示不同的VLAN。这样，**支持VLAN的交换机**在转发帧时，不仅会关注MAC地址，还会考虑到VLAN tag字段。VLAN tag中包含了TPIDPCP、CFI、VID，其中VID（VLAN ID）部分用来具体指出帧时属于哪个VLAN的。VID占12位，所以其取值范围为0到4095。下图演示了一个多交换机下VLAN划分的例子。
对着图看下面这段话。
![enter description here][58]
介绍一下交换机的access端口和trunk端口。图中，
- Port1、Port2、Port5、Port6、Port7、Port8为access端口，每一个access端口都会分配一个VLAN ID，标示它所连接的设备属于哪一个VLAN。
当数据帧从外界通过access端口进入交换机时，数据帧原来是不带tag的，access端口给数据帧打上tag（VLAN ID即为access端口所分配的VLAN ID），有tag就不能更改了，也不再重新打了；当数据帧从交换机内部通过access端口发送时，数据帧的VLAN ID必须和access端口的VLAN ID一致，access端口接收此帧，接着access端口将帧的tag信息去掉，再发送出去。
- Port3、Port4为trunk端口，trunk端口不属于某个特定的VLAN，而是交换机和交换机之间多个VLAN的通道。trunk端口声明了一组VLAN ID，表明只允许带有这些VLAN ID的数据帧通过，从trunk端口进入和出去的数据帧都是带tag的（不考虑默认VLAN的情况）。PC1和PC3属于VLAN100，PC2和PC4属于VLAN200，所以PC1和PC3处于同一个二层网络中，PC2和PC4处在同一个二层网络中。尽管PC1和PC2连接在同一台交换机中，但它么之间的通信是需要经过路由器的。

在这个例子中，VLAN tag是如何发挥作用的呢？当PC1向PC3发送数据时，PC1将IP包封装在以太帧中，帧的目的MAC地址为PC3的地址，此时帧并没有tag信息。当帧到达Port1时，Port1给帧打上tag（VID=100），帧进入switch1，然后帧通过Port3、Port4到达Switch2（Port3、Port4允许VLAN ID为100/200的帧通过）。在switch2中，Port5所标记的VID和帧相同，MAC地址也匹配，帧就发送到Port5上，Port5将帧的tag信息去掉，然后发给PC3。由于PC2、PC4与PC1的VLAN不同，因此收不到PC1发出的帧。

本节详细介绍如何使用pipework实现Docker容器的VLAN划分。
### 2.5.1 使用OVS对单主机上Docker容器的VLAN划分（最后是同主机且都在一个子网范围，A、C容器能互通，B、D容器能互通）
在Docker默认网络模式下，所有的容器都连在docker0网桥上。docker0网桥是普通的Linux网桥，不支持VLAN功能，为了方便操作，使用Open vSwitch代替docker0进行VLAN划分。

> Open vSwitch介绍：
> Open vSwitch是一个开源的虚拟交换机软件，有点儿像Linux中的网桥bridge，但功能复杂得多。Open vSwitch的网桥可以直接建立多种通信通道（如隧道[Open vSwitch with  GRE/VxLan]，），这些通道的建立可以很容易地通过OVS的配置命令实现。

如图是一个在一台主机上进行Docker容器VLAN划分的例子。
![enter description here][59]
为了演示隔离效果，图中4个容器都在同一个IP网段中，但实际上它们是二层隔离的两个网络，有不同的广播域。未完成上图的配置，我们在主机A上做如下操作：
```shell?linenums
#在主机A上创建4个Docker容器：con1、con2、con3、con4
$ docker run -itd --name con1 ubuntu /bin/bash
$ docker run -itd --name con2 ubuntu /bin/bash
$ docker run -itd --name con3 ubuntu /bin/bash
$ docker run -itd --name con4 ubuntu /bin/bash

#使用pipework将con1、con2划分到一个VLAN中，使用ovs0网桥，不存在则创建
$ pipework ovs0 con1 192.168.0.1/24 @100
$ pipework ovs0 con2 192.168.0.2/24 @100

#使用pipework将con3、con4划分到一个VLAN中
$ pipework ovs0 con3 192.168.0.3/24 @200
$ pipework ovs0 con4 192.168.0.4/24 @200
```
pipework配置完成后，每个容器都多了一块eth1网卡，eth1连在ovs0网桥上，并且进行了VLAN的隔离。和之前一样，通过`nc`命令测试各容器之间的连通性时发现，con1和con2可以相互通信，但与con3和con4隔离。如此一来，一个简单的VLAN隔离容器网络就完成了。

使用Open vSwitch配置VLAN比较简单，如创建access端口和trunk端口使用如下命令：
```shell?linenums
#在ovs0网桥上增加两个端口port1、port2
$ ovs-vsctl add-port ovs0 port1 tag=100
$ ovs-vsctl add-port ovs0 port2 trunk=100,200
```

**在向Open vSwitch中添加端口时，若不添加任何限制，此端口则转发所有帧。**

### 2.5.2 使用OVS对多主机Docker容器的VLAN划分
多主机VLAN的情况下，肯定有属于同一VLAN但又在不同主机上的容器，因此多主机VLAN划分的前提是跨主机通信。前面介绍过桥接和直接路由两种跨主机通信的方式，要使不同主机上的容器处于同一VLAN，就只能采用桥接方式。首先用桥接的方式将所有容器连接在一个逻辑交换机上，再根据情况进行VLAN的划分。桥接需要将主机的一块网卡桥接到容器所连接的Open vSwitch网桥上（桥接要求必须两个网卡或以上，eth0网卡连外网，eth1网卡连接docker0网桥），这就需要一块额外的网卡eth1来完成，桥接的网卡需要开启混杂模式。下图演示了一个多主机的Docker容器VLAN划分的例子。
![enter description here][60]
这里，我们将不同VLAN的容器的设在同一个子网中，仅仅是为了演示隔离效果。如上图中，host1上的con1和host2上的con3属于VLAN100，con2和con4属于VLAN200.由于会有VLAN ID为100和VLAN ID为200的帧通过，**物理交换机上连接host1和host2的端口应设置为trunk端口**。host1和host2上eth1没有设置VLAN的限制（trunk），是允许所有帧通过的。完成上图例子需要作如下操作：
```shell
#在host1上
$ docker run -itd --name con1 ubuntu /bin/bash
$ docker run -itd --name con2 ubuntu /bin/bash
$ pipework ovs0 con1 192.168.0.1/24 @100
$ pipework ovs0 con1 192.168.0.2/24 @200
$ ovs-vsctl add-port ovs0 eth1;

#在host2上
$ docker run -itd --name con3 ubuntu /bin/bash
$ docker run -itd --name con4 ubuntu /bin/bash
$ pipework ovs0 con3 192.168.0.3/24 @100
$ pipework ovs0 con4 192.168.0.4/24 @200
#此端口则转发所有帧
$ ovs-vsctl add-port ovs0 eth1;

```
完成之后，再通过nc命令测试实验效果即可。

## 2.5 OVS隧道模式
上面讲到的跨主机通信方式：桥接和直接路由，有一个局限——要求主机在同一个子网中。当基础设施的规模足够大时，这种局限性就会暴露出来，比如两个数据中心的Docker容器需要通信时，这两种方法就会失效。上节介绍的主流隔离技术VLAN，VLAN也有诸多限制。首先，VLAN是在二层帧头上做文章，也要求主机在同一个子网中。其次，提到过VLAN ID只有12个比特单位，即可用的数量为4000个左右，这样的数量对于公有云或大型虚拟化环境而言捉襟见肘。除此之外，VLAN配置比较繁琐且不够灵活。这些问题就是当前云计算所面临的网络考验，目前比较普遍的解决方法是使用Overlay的虚拟化网络技术。

### 2.5.1 Overlay技术模型
Overlay网络其实就是隧道技术（叠加网络），即将一种网络协议包装在另一种协议中传输的技术。如果有两个使用IPv6的站点之间需要通信，而它们之间的网络使用IPv4协议，这时就需要将IPv6的数据包装在IPv4数据包中进行传输。隧道被广泛用于连接因使用不同网络而被隔离的主机和网络，使用隧道技术搭建的网络就是所谓的Overlay网络。它能有效地覆盖在基础网络之上，该模型可以很好地解决跨网络Docker容器实现二层通信的要求。

在普通的网络传输中，源IP地址和目的IP地址是不变的，而二层的帧头在每个路由器节点上都会改变，这是TCP/IP协议所作的规定。那么，如何使两个中间隔离了因特网的主机像连在同一个台交换机上一样通信呢？如果将以太网帧封装在IP包中，通过中间的因特网，最后传输到目的网络中再解封装，这样就可以保证二层帧头在传输过程中不改变，这也就是早期Ethernet in IP的二层Overlay技术。至于多租户隔离问题，解决思路是将不同租户的流量放在不同的隧道中进行隔离。用于封装传输数据的协议也会有一个类似VLAN ID的标识，以区别不同的隧道。下图演示了多租户环境下Overlay技术的应用。
![enter description here][61]
当前主要的Overlay技术有`VXLAN（Virtual Extensible LAN）`和`NVGRE（Network Virtualization using Generic Routing Encapsulation）`。
- **VXLAN**是将以太网报文封装在UDP传输层上的一种隧道转发模式，它采用24位比特标示二层网络分段，称为VNI（VXLAN Network Identifier），类似于VLAN ID的作用。
- **NVGRE**同VXLAN类似，它使用GRE的方法来打通二层与三层之间的通路，采用24位比特的GRE key来作为网络标识（TNI）。
 
本节主要使用NVGRE来演示Docker容器的跨网络通信。
使用VXLAN模式，看文章《ovs vxlan学习 - 西邮记的专栏 - 博客频道 - CSDN.NET》

### 2.5.2 GRE简介
NVGRE使用GRE协议来封装需要传送的数据，下面先了解一下GRE协议，GRE协议可以用来封装任何其他网络层的协议。下面直接通过一个VPN的例子来演示GRE封装过程，如下图所示，一个公司有两个处于不同地方的办公地点需要通信。两个地点的主机都处在NAT转换之下，因此两地的主机并不能直接进行ping或ssh操作。如何才能使两个办公地点相互通信呢？通过在双方路由器上配置GRE隧道就可实现该目的。
![enter description here][62]
- 首先在路由器上配置一个GRE隧道的网卡设备。
- 添加一条静态路由，将目的地址为192.168.x.0/24的包通过上面配置的隧道设备发送出去。
- 配置完成后，分析一下从IP地址为192.168.1.1/24的主机A ping IP地址为192.168.2.1/24的主机B的过程。主机A构造好IP包后，通过查看路由表发现目的地址和本身不在同一个子网中，要将其转发到默认网关192.168.1.254.主机A将IP包封装在以太网帧中，源MAC地址为本身网卡的MAC地址，目的MAC地址为网关的MAC地址，数据格式如下图所示。
![enter description here][63]
网关路由器收到数据帧后，去掉帧头，将IP包取出来，匹配目的IP地址和自身的路由表，确定包需要从GRE隧道设备发出，这就对着IP包做GRE封装，即加上GRE协议头部。封装完成后，该包是不能直接发往互联网的，需要生成新的IP包作为载体来传输GRE数据包，新IP包的源地址为1.1.1.1，目的地址为2.2.2.2.当然，这个IP包会装载新的广域网二层帧中发出去，数据格式如下图所示。
![enter description here][64]
在传输的过程中，中间的节点仅能看到最外层的IP包。当IP包到达2.2.2.2的路由器后，路由器将外层IP头部和GRE头部去掉，得到原始的IP数据包，再将其发往192.168.2.1。对于原始IP包而言，两个路由器之间的栓出国城就如同单条链路上的一跳。在这个例子中，GRE协议封装的是IP包，实现了一个VPN的功能。
> 好像和我平常用的不一样，一会仔细研究一下。

### 2.5.3 GRE实现Docker容器跨网络通信（容器在同一个子网中）
GRE功能强大，可以实现真正容器间跨主机通信，那么我们该如何使用它呢？目前比较普遍的方法是结合Open vSwitch使用。Open vSwitch是一个功能强大的虚拟交换机，支持GRE、VXLAN等协议。
下面两台主机处在不同的网络中，接着在两台主机中间建立GRE隧道，就可以使它上面的Docker容器间进行通信，如下图。
![enter description here][65]
如上图，两台Ubuntu的主机host1和host2，host1的IP为10.10.103.91/24，host2的IP为10.10.105.235/24。为了解决两台主机上IP地址冲突的问题，还是使用--fixed-cidr参数将不同主机上的Docker容器地址限定在不同的范围中。ovs0为Open vSwitch网桥，用来创建GRE隧道，并与Docker自带的网桥docker0桥接在一起，如此一来，连接在docker0上的容器就可以通过ovs0的隧道到达另一台主机。具体操作如下：
```shell?linenums
#在host1上作如下操作
#配置--fix-cidr参数，重启docker
$ echo 'DOCKER_OPTS="--fixed-cidr=172.17.1.1/24"' >> /etc/default/docker
$ service docker restart

#创建ovs0网桥，并将ovs0连在docker0上
$ ovs-vsctl add-br ovs0
#将docker0网桥桥接到ovs0
$ brctl addif docker0 ovs0

#在ovs0上创建GRE隧道
$ ovs-vsctl add-port ovs0 gre0 -- set interface gre0 type=gre options:remote_ip=10.10.105.235


#在host2上做如下操作
$ echo 'DOCKER_OPTS="--fixed-cidr=172.17.2.1/24"' >> /etc/default/docker
#为避免和host1上的docker0的IP冲突，修改docker0的IP
$ ifconfig docker0 172.17.42.2/16
$ service docker restart

#创建ovs网桥，并将ovs0连接在docker0上
$ ovs-vsctl add-br ovs0
$ brctl addif docker0 ovs0

#在ovs0上创建GRE隧道
$ ovs-vsctl add-port ovs0 gre0 -- set interface gre0 type=gre options:remote_ip=10.10.103.91
```
创建ovs0网桥后，在主机上通过ifconfig命令可以看到一块名为ovs0的网卡。该网卡就是ovs0网桥自带的一个类型为internal的端口，就如同普通Linux网桥也有一个同名的端口一样，Linux主机将其作为一块虚拟网卡使用。将这块网卡加入docker0后，就将docker0网桥和ovs0网桥级联了起来。
配置完成后，两台主机上的容器就可以通过GRE隧道通信了，下面来验证一下。
```shell?linenums
#在host1上启动一个容器con1
$ docker run -it --rm --name con1 ubuntu /bin/bash
#在con1容器中，操作如下
$ nc -l 172.17.2.1 9000

#在host2上启动一个容器con2
$ docker run -it --rm --name con1 ubuntu /bin/bash
#在con2容器中，操作如下
$ nc -l 172.17.1.1 9000
hi!
```
在con1可以显示con2上输入的内容，表示两台容器可以正常通信。
这里与桥接方法一模一样，尽管不同主机上的容器IP有不同的范围，但它们还是属于同一个子网（172.17.0.0/16）。con1向con2发送数据时，会发送ARP请求获取con2的MAC地址。ARP请求会被docker0网桥洪泛到所有端口，包括和ovs0网桥相连的ovs0端口。ARP请求到达ovs0网桥后，继续洪泛，通过gre0隧道叨叨host2上的ovs0中，最后到达con2。host1和host2处在不同的网络中，该ARP请求是如何跨越中建网络到达host2的呢？ARP请求经过gre0时，会首先加上一个GRE协议的头部，然后再加上一个源地址为10.10.103.91、目的地址为10.10.105.235的IP协议的头部，在发送给host2.这里GRE协议封装的是二层以太网帧，而非三层IP数据包。con1获取到con2的MAC地址之后，就可以向它发送数据，发送数据包的流程和发送ARP请求的流程类似。只不过docker0和ovs0会学习到con2的MAC地址该从哪个端口发送出去，而无需洪泛到所有端口。

### 2.5.4 GRE实现Docker容器跨网络通信（容器在不同子网中）
前面直接路由的方法中，不同主机上的容器是不同的子网中，而不是在同一个子网中。使用Open vSwitch的隧道模式也可以实现此网络模型，如下图：
![enter description here][66]
图中有两台Ubuntu的主机host1和host2，host1的IP地址为10.10.103.91/24，host2的IP地址为10.10.105.235/24。host1的Docker容器在172.17.1.0/24子网中，host2上的容器在172.17.2.0/24子网中。由于两个主机不在同一个子网中，容器间通信不能再使用直接路由的方式，而需依赖Open vSwitch建立的GRE隧道进行，配置如下：
```shell?linenums
#在host1上做如下操作
#配置docker0，使Docker容器的IP在172.17.1.0/24网络中
$ ifconfig docker0 172.17.1.254/24
$ service docker restart

#创建ovs0网桥，并将ovs0连在docker0上
$ ovs-vsctl add-br ovs0
$ brctl addif docker0 ovs0

#在ovs0上创建一个internal类型的端口rou0，并分配一个不引起冲突的私有IP
$ ovs-vsctl add-port ovs0 rou0 -- set interface rou0 type=internal
$ ifconfig rou0 192.168.1.1/24
#将通往Docker容器的流量路由到rou0
$ route add -net 172.17.0.0/16 dev rou0

#创建GRE隧道
$ ovs-vsctl add-port ovs0 gre0 -- set interface gre0 type=gre options:remote_ip=10.10.103.91

#删除Docker创建的iptables规则
$ iptables -t nat -D POSTROUTING -s 172.17.1.0/24 ! -o docker0 -j MASQUERADE
#创建自己的规则
$ iptables -t nat -A POSTROUTING -s 172.17.0.0/16 -o eth0 -j MASQUERADE


#在host2上做如下操作
#配置docker0，使Docker容器的IP在172.17.2.0/24网络中
$ ifconfig docker0 172.17.2.254/24
$ service docker restart

#创建ovs0网桥，并将ovs0连在docker0上
$ ovs-vsctl add-br ovs0
$ brctl addif docker0 ovs0

#在ovs0上创建一个internal类型的端口rou0，并分配一个不引起冲突的私有IP
$ ovs-vsctl add-port ovs0 rou0 -- set interface rou0 type=internal
$ ifconfig rou0 192.168.1.1/24
#将通往Docker容器的流量路由到rou0
$ route add -net 172.17.0.0/16 dev rou0

#创建GRE隧道
$ ovs-vsctl add-port ovs0 gre0 -- set interface gre0 type=gre options:remote_ip=10.10.105.235

#删除Docker创建的iptables规则
$ iptables -t nat -D POSTROUTING -s 172.17.2.0/24 ! -o docker0 -j MASQUERADE
#创建自己的规则
$ iptables -t nat -A POSTROUTING -s 172.17.0.0/16 -o eth0 -j MASQUERADE
```
在两台主机上分别创建一个Docker容器验证容器间的通信。
```shell?linenums
#在host1上启动容器con1
$ docker run -it --name con1 ubuntu /bin/bash
#在con1容器中
$ nc -l 9000

#在host2上启动容器con2
$ docker run -it --name con2 ubuntu /bin/bash
#在con2容器中
$ nc -w 1 -v 172.17.1.1 9000
Connection to 172.17.1.1 9000 port [tcp/*] succeeded!
```
### 2.5.5 多租户环境下的GRE网络
没有深入研究

![enter description here][67]
![enter description here][68]
![enter description here][69]
![enter description here][70]
![enter description here][71]
![enter description here][72]
![enter description here][73]
![enter description here][74]
![enter description here][75]
![enter description here][76]
![enter description here][77]

## 2.6 隧道方案Flannel Overlay网络
Flannel能实现跨主机通信，并专门提供了对Docker的支持，因为其能实现一下两点：
（1）能协助K8s，给每一个Node上的Docker容器分配互相不冲突的IP地址。利用etcd，确保每个机器的Docker都使用一个不重复的子网段，指定docker daemon启动参数`--bip=172.17.18.1/24`
（2）它能在这些IP地址之间建立一个叠加网络（Overlay Network），通过这个叠加网络，将数据包原封不动地传递到目标容器内。
看下图，了解Flannel是如何实现这两点的。
![enter description here][78]
通过上图看到，首先Flannel创建了一个flannel0的网桥，而且这个网桥的一端连接docker0网桥，另一端连接一个叫做flanneld的服务进程。

flanneld首先上连etcd，利用etcd来管理可分配的IP地址段资源，同时监控etcd中每个Pod的实际地址，并在内存中建立了一个Pod节点路由表；
下连docker0和物理网络，使用内存中的Pod节点路由表，将docker0发给它的数据包包装起来，利用物理网络的连接将数据包投递到目标flanneld上，从而实现Pod到Pod之间的直接的地址通信。

Flannel之间的底层通信协议的可选余地很多，有UDP、VxLan、AWS VPC等多种方式，只要能通到对端的Flannel就可以了。源flanneld加包，目标flanneld解包，最终docker0看到的就是原始的数据，非常透明，根本感觉不到中间Flannel的存在。常用的是UDP。

> Flannel完美地实现了对K8s网络的支持，但是它引入了多个网络组件，在网络通信时需要转到flannel0网络接口，再转到用户态的flanneld程序，到对端后还需要走这个过程的反过程，所以会引入一些网络的时延损耗。另外，Flannel模型缺省地使用了UDP作为底层传输协议，UDP本身是非可靠协议，虽然两端的TCP实现了可靠传输，但在大流量、高并发应用场景下还需要反复测试。

关于Flannel的几种方案对比，看文章《一个适合 Kubernetes 的最佳网络互联方案 - DockOne.io》
## 2.7 隧道方案Weave Overlay网络
## 2.8 MacVlan+VLAN构建网络
看文章《通过MacVLAN实现Docker跨宿主机互联 - 推酷》和《linux 网络虚拟化： macvlan - 推酷》和《通过MacVLAN实现Docker跨宿主机互联》

## 2.9 路由方案Calico网络
关于Calico介绍，使用，看文章《DockOne微信分享（六十六）： Docker网络方案初探，重点推荐Calico》和《探索Kubernetes的网络原理及方案》，该篇文章具有总结性，总结所有跨主机方案，包括各种Overlay，和直接路由等方案。

## 2.10 Docker1.9后内置Overlay网络（对比所有的Overlay技术）
内置跨主机的网络通信一直是Docker备受期待的功能，在1.9版本之前，社区中就已经有许多第三方的工具或方法尝试解决这个问题，例如Macvlan、Pipework、Flannel、Weave等。虽然这些方案在实现细节上存在很多差异，但其思路无非分为两种：二层VLAN网络和Overlay网络。

简单来说，二层VLAN网络的解决跨主机通信的思路是把原先的网络架构改造为互通的大二层网络，通过特定网络设备直接路由，实现容器点到点的之间通信。这种方案在传输效率上比Overlay网络占优，然而它也存在一些固有的问题。
1. 这种方法需要二层网络设备支持，通用性和灵活性不如后者；
2. 由于通常交换机可用的VLAN数量都在4000个左右，这会对容器集群规模造成限制，远远不能满足公有云或大型私有云的部署需求；
3. 大型数据中心部署VLAN，会导致任何一个VLAN的广播数据会在整个数据中心内泛滥，大量消耗网络带宽，带来维护的困难。

相比之下，Overlay网络是指在不改变现有网络基础设施的前提下，通过某种约定通信协议，把二层报文封装在IP报文之上的新的数据格式。这样不但能够充分利用成熟的*IP路由协议*进程数据分发，而且在Overlay技术中采用扩展的隔离标识位数，能够突破VLAN的4000数量限制，支持高达16M的用户，并在必要时可将广播流量转化为组播流量，避免广播数据泛滥。因此，Overlay网络实际上是目前最主流的容器跨节点数据传输和路由方案。

在Docker的1.9中版本中正式加入了官方支持的跨节点通信解决方案，而这种内置的跨节点通信技术正是使用了Overlay网络的方法。

**说到Overlay网络，许多人的第一反应便是：低效，这种认识其实是带有偏见的。Overlay网络的实现方式可以有许多种，其中IETF（国际互联网工程任务组）制定了三种Overlay的实现标准，分别是：虚拟可扩展LAN（VXLAN）、采用通用路由封装的网络虚拟化（NVGRE）和无状态传输协议（SST），其中以VXLAN的支持厂商最为雄厚，可以说是Overlay网络的事实标准。**

**而在这三种标准以外还有许多不成标准的Overlay通信协议，例如Weave、Flannel、Calico等工具都包含了一套自定义的Overlay网络协议（Flannel也支持VXLAN模式），这些自定义的网络协议的通信效率远远低于IETF的标准协议[5]，但由于他们使用起来十分方便，一直被广泛的采用而造成了大家普遍认为Overlay网络效率低下的印象。然而，根据网上的一些测试数据来看，采用VXLAN的网络的传输速率与二层VLAN网络是基本相当的。**

解除了这些顾虑后，一个好消息是，Docker内置的Overlay网络是采用IETF标准的VXLAN方式，并且是VXLAN中普遍认为最适合大规模的云计算虚拟化环境的SDN Controller模式。

到目前为止一切都是那么美好，大家是不是想动手尝试一下了呢？

且慢，待我先稍泼些冷水。在许多的报道中只是简单的提到，这一特性的关键就是Docker新增的『overlay』类型网卡，只需要用户用『docker networkcreate』命令创建网卡时指定『–driver=overlay』参数就可以。看起来就像这样：
```shell?linenums
$ docker network create –driver=overlay ovr0
```
但现实的情况是，直到目前为止，Docker的Overlay网络功能与其Swarm集群是紧密整合的，因此为了使用Docker的内置跨节点通信功能，最简单的方式就是采纳Swarm作为集群的解决方案。这也是为什么Docker 1.9会与Swarm1.0同时发布，并标志着Swarm已经Product-Ready。此外，还有一些附加的条件：
1. 所有Swarm节点的Linux系统内核版本不低于3.16
2. 需要一个额外的配置存储服务，例如Consul、Etcd或ZooKeeper
3. 所有的节点都能够正常连接到配置存储服务的IP和端口
4. 所有节点运行的Docker后台进程需要使用『–cluster-store』和『-–cluster-advertise』参数指定所使用的配置存储服务地址
我们先不解释为什么必须使用Swarm，稍后大家很快就会发现原因。假设上述条件的1和3都是满足的，接下来就需要建立一个外部配置存储服务，为了简便起见暂不考虑高可用性，可以采用单点的服务。
具体步骤，看文章《Docker 1.9的新网络特性，以及Docker1.9后内置Overlay详解 - 推酷》的第二部分【Docker的内置Overlay网络】
另外看文章《DockOne微信分享（六十六）： Docker网络方案初探，重点推荐Calico》和《探索Kubernetes的网络原理及方案》，该篇文章具有总结性，总结所有跨主机方案，包括各种Overlay，也有Docker+swarm，和直接路由等方案。
## 2.11 如何使用DNS、域名方式让访问服务
看文章《DockOne技术分享（十）：跨主机的，使用dns自动获取IP方式 --link - DockOne.io》


# 3. 单机Docker容器网络详讲
> Docker实质上是汇集了linux容器（6种namespaces）、cgroups以及“叠加”类文件系统等多种核心技术的一种复合技术。 其默认容器网络的建立和控制是一种结合了network namespace、iptables、linux网桥、route table等多种Linux内核技术的综合方案。本章主要针对单机Docker容器网络，目的是了解Docker容器网络中==容器与容器间通信==、==容器与宿主机间通信==、==容器与宿主机所在的物理网络中主机通信==、==容器网络控制==等机制。同时稍带利用工具对Docker容器网络的网络性能做初步测量，通 过直观数据初步评估容器网络的适用性。

**结合下面例子会彻底理解网络命名空间，Veth设备对，网桥，Iptables/Netfilter、路由**


1) 查看Docker启动后的系统情况

​ 我们已经知道，Docker网络在bridge模式下Docker Daemon启动时创建docker0网桥，并在网桥使用的网段为容器分配IP。让我们看看实际的操作：

​ 在刚刚启动Docker Daemon，并且没有启动任何容器的时候，网络协议栈的配置情况如下：
```shell?linenums
[root@localhost ~]# systemctl start docker
[root@localhost ~]# ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 52:54:00:88:15:b6 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 85983sec preferred_lft 85983sec
    inet6 fe80::5054:ff:fe88:15b6/64 scope link
       valid_lft forever preferred_lft forever
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN
    link/ether 02:42:1e:31:a3:71 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 scope global docker0
       valid_lft forever preferred_lft forever
[root@localhost ~]# iptables-save
# Generated by iptables-save v1.4.21 on Fri Jun  2 02:33:16 2017
*nat
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [0:0]
:OUTPUT ACCEPT [2:144]
:POSTROUTING ACCEPT [2:144]
:DOCKER - [0:0]
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A DOCKER -i docker0 -j RETURN
COMMIT
# Completed on Fri Jun  2 02:33:16 2017
# Generated by iptables-save v1.4.21 on Fri Jun  2 02:33:16 2017
*filter
:INPUT ACCEPT [47:2735]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [25:2268]
:DOCKER - [0:0]
:DOCKER-ISOLATION - [0:0]
-A FORWARD -j DOCKER-ISOLATION
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A DOCKER-ISOLATION -j RETURN
COMMIT
# Completed on Fri Jun  2 02:33:16 2017
[root@localhost ~]#
```
可以看到，Docker创建了docker0网桥，并添加了Iptables规则。docker0网桥和Iptables规则都处于root命名空间中。解读这些规则会发现，在没有启动任何容器时候，如果启动了Docker Daemon，那么它就已经做好了通信的准备。解释这些规则：

在nat表中：

第一、二条匹配生效后，都会继续执行DOCKER链，而此时DOCKER链为空，所以前两条只是做了个框架，并没有实际效果。后面再启动做端口映射了得容器时，会用到DOCKER链。

第三条关系着Docker容器和外界的通信，含义是将源地址为172.17.0.0/16的数据包（即Docker容器发出的数据包），当不是从docker0网卡发出时做SNAT（源地址转换，将IP包的源地址替换为相应网卡的地址）。这样一来，从Docker容器访问外界的流量，在外部看来就是从宿主机上发出的，外界感觉不到Docker容器的存在。
含义是，若本地发出的数据包不是发往docker0的，即是发往主机之外的设备的，都需要进行动态地址修改（MASQUERADE），将源地址从容器的地址（172段）修改为宿主机网卡的IP地址，之后就可以发送给外面的网络了。

在filter表中：

第二条也是一个框架，因为后继的DOCKER链是空的。

第三条是说，如果接收到的数据包属于以前已经建立好的连接，那么允许直接通过。这样接收到的数据包自然又走到docker0，并中转到相应的容器。

第四条是说，docker0发出的包，如果需要Forward到非docker0的本地IP地址的设备，则是允许的，这样，docker0设备的包就可以根据路由规则中转到宿主机的网卡设备，从而访问外面的网络。

第五条是说，docker0的包还可以中转给docker0本身，即连接在docker0网桥上的不同容器之间的通信也是允许的。

除了这些Netfilter的设置，Linux的ip_forward功能也被Docker Daemon打开了：
```shell?linenums
[root@localhost ~]# cat /proc/sys/net/ipv4/ip_forward
1
[root@localhost ~]#
```
另外，我们还可以看到刚刚启动Docker后的Route表，和启动前没有什么不同：
```shell?linenums
[root@localhost ~]# ip route
default via 10.0.2.2 dev eth0  proto static  metric 100
10.0.2.0/24 dev eth0  proto kernel  scope link  src 10.0.2.15  metric 100
172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1
[root@localhost ~]#
```
2) 查看容器启动后的情况（容器无端口映射）

刚才我们看了Docker服务启动后的网络情况。现在，我们启动一个Registry容器后（不使用任何端口镜像参数），看一下网络堆栈部分相关的变化：
```shell?linenums
[root@localhost ~]# docker run --name register -d registry
93729ce2a2454461ee497384a1b9054f26e9bda035d26d671c90dbfc2de3081a
[root@localhost ~]# ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 52:54:00:88:15:b6 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 84632sec preferred_lft 84632sec
    inet6 fe80::5054:ff:fe88:15b6/64 scope link
       valid_lft forever preferred_lft forever
3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP
    link/ether 02:42:1e:31:a3:71 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 scope global docker0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:1eff:fe31:a371/64 scope link
       valid_lft forever preferred_lft forever
5: veth4652561@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP
    link/ether 26:83:1e:9f:82:bb brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet6 fe80::2483:1eff:fe9f:82bb/64 scope link
       valid_lft forever preferred_lft forever
[root@localhost ~]#
```
Docker Daemon为容器创建了个网络命名空间。我们看一下这个网络命名空间的Veth设备对是哪个
```shell?linenums
[root@localhost ~]# ethtool -S veth4652561
NIC statistics:
     peer_ifindex: 4
```
对应的是容器内网桥索引。
```shell?linenums
[root@localhost ~]# iptables-save
# Generated by iptables-save v1.4.21 on Fri Jun  2 03:01:33 2017
*nat
:PREROUTING ACCEPT [1:100]
:INPUT ACCEPT [1:100]
:OUTPUT ACCEPT [171:12158]
:POSTROUTING ACCEPT [171:12158]
:DOCKER - [0:0]
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A DOCKER -i docker0 -j RETURN
COMMIT
# Completed on Fri Jun  2 03:01:33 2017
# Generated by iptables-save v1.4.21 on Fri Jun  2 03:01:33 2017
*filter
:INPUT ACCEPT [3600:10945550]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [3196:192735]
:DOCKER - [0:0]
:DOCKER-ISOLATION - [0:0]
-A FORWARD -j DOCKER-ISOLATION
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A DOCKER-ISOLATION -j RETURN
COMMIT
# Completed on Fri Jun  2 03:01:33 2017
[root@localhost ~]#
[root@localhost ~]#
[root@localhost ~]# ip route
default via 10.0.2.2 dev eth0  proto static  metric 100
10.0.2.0/24 dev eth0  proto kernel  scope link  src 10.0.2.15  metric 100
172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1
[root@localhost ~]#
```
可以看到：

（1）宿主机器上的Netfilter和路由表都没有变化，说明在不进行端口映射时，Docker的默认网络是没有特殊处理的。相关的NAT和FILTER两个Netfilter链都还是空的。

（2）宿主机上的Vet对已经建立，并可以连接到了容器内。

我们再进入刚刚启动的容器内，看看网络栈是什么情况。容器内部的IP地址和路由如下：
```shell?linenums
[root@localhost ~]# docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
93729ce2a245        registry            "/entrypoint.sh /etc/"   10 minutes ago      Up 10 minutes       5000/tcp            register
[root@localhost ~]# docker exec -ti 93729ce2a245 /bin/sh
/ # ip route
default via 172.17.0.1 dev eth0
172.17.0.0/16 dev eth0  src 172.17.0.2
/ # ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
4: eth0@if5: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.2/16 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe11:2/64 scope link
       valid_lft forever preferred_lft forever
/ #
```
我们可以看到，默认挺值得回路设备lo已经被启动，外面宿主机连接进来的Veth设备也被命名成了eth0，并且也可以配置了地址172.17.0.10。

路由信息表包含了一条到docker0的子网络和一条到docker0的默认路由。

3）查看容器启动后的情况（容器有端口映射）

下面，我们用端口映射的命令启动registry：
```shell?linenums
[root@localhost ~]# docker run --name register -d -p 1180:5000 registry
d17de890f341be7feaab1e6557f51f744fe863a30787953e8bd4870fda820f7d
[root@localhost ~]#
```
在启动后查看Iptables的变化。
```shell?linenums
[root@localhost ~]# iptables-save
# Generated by iptables-save v1.4.21 on Fri Jun  2 03:12:09 2017
*nat
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
:DOCKER - [0:0]
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A POSTROUTING -s 172.17.0.2/32 -d 172.17.0.2/32 -p tcp -m tcp --dport 5000 -j MASQUERADE
-A DOCKER -i docker0 -j RETURN
-A DOCKER ! -i docker0 -p tcp -m tcp --dport 1180 -j DNAT --to-destination 172.17.0.2:5000
COMMIT
# Completed on Fri Jun  2 03:12:09 2017
# Generated by iptables-save v1.4.21 on Fri Jun  2 03:12:09 2017
*filter
:INPUT ACCEPT [15:852]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [8:608]
:DOCKER - [0:0]
:DOCKER-ISOLATION - [0:0]
-A FORWARD -j DOCKER-ISOLATION
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A DOCKER -d 172.17.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 5000 -j ACCEPT
-A DOCKER-ISOLATION -j RETURN
COMMIT
# Completed on Fri Jun  2 03:12:09 2017
[root@localhost ~]#
```
从新增的规则可以看出，Docker服务在NAT和FILTER两个表内添加的的两个DOCKER子链都是给端口映射用的。例如本例中，我们需要把外部宿主机的1180端口映射到容器的5000端口，所以外界访问Docker容器是通过iptables做DNAT（目的地址转换）实现的。通过前面的分析我们知道，无论是宿主机接收到的还是宿主机本地协议发出的，目标地址是本地IP地址的包都会经过NAT表中的DOCKER子链。Docker为每一个端口映射都在这个链上增加了到实际容器目标地址和目标端口的转换。

​ 经过这个DNAT的规则修改后的IP包，会重新经过路由模块的判断进行转发。由于目标地址和端口已经是容器的地址和端口，所以数据自然被送到了docker0上，从而送到对应的容器内部。

​ 当然在Forward时，也需要在Docker子链中添加一条规则，如果目标端口和地址是指定容器的数据，则允许通过。同时Docker的forward规则默认允许所有的外部IP访问容器，可以通过在filter的DOCKER链上添加规则来对外部的IP访问做出限制，如只允许源IP为8.8.8.8的数据包访问容器，需要添加如下规则：
```shell?linenums
iptables -I DOCKER -i docker0 ! -s 8.8.8.8 -j DROP
```

​ 在Docker按照端口映射的方式启动容器时，主要的不同就是上述Iptables部分。而容器内部的路由和网络设备，都和不做端口映射时一样，没有任何变化。

不仅仅是外界间通信，Docker容器之间互相通信也受到iptables规则限制。同一台宿主机上的Docker容器默认都连在docker0网桥上，它们属于一个子网，这是满足通信的第一步。同时，Docker daemon会在filter的FORWARD链中增加一条ACCEPT的规则（-icc=true）：
```shell?linenums
-A FORWARD -i docker0 -o docker0 -j ACCEPT
```
这是满足通信的第二步。当Docker daemon启动参数-icc（表示是否允许容器间互相通信）设置为false，以上规则会被设置为DROP，Docker容器间的相互通信就被禁止，这种情况下，想让两个容器通信就需要在docker run时使用--link选项。

要想看详细的数据流流转（宿主机到容器，容器到容器，宿主机外部到容器里等等）情况，看文章【理解Docker单机容器网络 | Tony Bai】


  [1]: ./images/1497497050636.jpg
  [2]: ./images/1497497079367.jpg
  [3]: ./images/1497497090809.jpg
  [4]: ./images/1497497119734.jpg
  [5]: ./images/1497497140470.jpg
  [6]: ./images/1497497161540.jpg
  [7]: ./images/1497497458402.jpg
  [8]: ./images/1497497476360.jpg
  [9]: ./images/1497497486753.jpg
  [10]: ./images/1497497578358.jpg
  [11]: ./images/1497497653424.jpg
  [12]: ./images/1497497675134.jpg
  [13]: ./images/1497509604481.jpg
  [14]: ./images/1497509649258.jpg
  [15]: ./images/1497509670118.jpg
  [16]: ./images/1497497711888.jpg
  [17]: ./images/1497717885328.jpg
  [18]: ./images/1497717912492.jpg
  [19]: ./images/1497497810458.jpg
  [20]: ./images/1497497820499.jpg
  [21]: ./images/1497497830912.jpg
  [22]: ./images/1497497843709.jpg
  [23]: ./images/1497497858843.jpg
  [24]: ./images/1497497951453.jpg
  [25]: ./images/1497497975985.jpg
  [26]: ./images/1497497991229.jpg
  [27]: ./images/1497498053429.jpg
  [28]: ./images/1497498071124.jpg
  [29]: ./images/1497498083688.jpg
  [30]: ./images/1497498092559.jpg
  [31]: ./images/1497498106784.jpg
  [32]: ./images/1497498116302.jpg
  [33]: ./images/1497498125107.jpg
  [34]: ./images/1497498134464.jpg
  [35]: ./images/1497498145047.jpg
  [36]: ./images/1497498154513.jpg
  [37]: ./images/1497498165003.jpg
  [38]: ./images/1497498176695.jpg
  [39]: ./images/1497498191194.jpg
  [40]: ./images/1497498228488.jpg
  [41]: ./images/1497498242777.jpg
  [42]: ./images/1497498253089.jpg
  [43]: ./images/1497498265104.jpg
  [44]: ./images/1497498286676.jpg
  [45]: ./images/1497498294736.jpg
  [46]: ./images/1497498304382.jpg
  [47]: ./images/1497498313637.jpg
  [48]: ./images/1497498323994.jpg
  [49]: ./images/1497519845453.jpg
  [50]: ./images/1497519858577.jpg
  [51]: ./images/1497520283658.jpg
  [52]: ./images/1497581340792.jpg
  [53]: ./images/1497582713829.jpg
  [54]: ./images/1497599407936.jpg
  [55]: ./images/1497601411449.jpg
  [56]: ./images/1497601475493.jpg
  [57]: ./images/1497603295642.jpg
  [58]: ./images/1497607016587.jpg
  [59]: ./images/1497609663256.jpg
  [60]: ./images/1497611110763.jpg
  [61]: ./images/1497621231945.jpg
  [62]: ./images/1497622235254.jpg
  [63]: ./images/1497624339743.jpg
  [64]: ./images/1497624714638.jpg
  [65]: ./images/1497625434341.jpg
  [66]: ./images/1497673406456.jpg
  [67]: ./images/1497683095622.jpg
  [68]: ./images/1497683129680.jpg
  [69]: ./images/1497683150142.jpg
  [70]: ./images/1497683171406.jpg
  [71]: ./images/1497683189614.jpg
  [72]: ./images/1497683206619.jpg
  [73]: ./images/1497683231252.jpg
  [74]: ./images/1497683241715.jpg
  [75]: ./images/1497683258323.jpg
  [76]: ./images/1497683286395.jpg
  [77]: ./images/1497683317934.jpg
  [78]: ./images/1497690733452.jpg
  