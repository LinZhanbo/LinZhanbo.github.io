---
title: 跨主机容器网络通信方案（精华总结） 
tags: SDN,容器网络
grammar_cjkRuby: true
---

如何让多主机的docker容器互通，分两种情况：
* 宿主机都必须在同一个子网情况下
	* 配置容器与宿主机在同一个网段。大二层网络
		* 利用虚拟网桥将Docker容器桥接到本地网络
		1. 在宿主机上创建一个普通虚拟网桥br0，在网桥br0添加eth0网卡。
		2. 创建一对veth设备对，一端添加到容器的网络命名空间中，重命名为eth0，并指定一个二层网络的IP。一段添加到网桥br0上。
例子：
![enter description here][1]
现在test1容器可以很方便的实现与本地主机相互访问。
```shell?linenums
pipework br0 test1 10.10.103.95/24@10.10.103.254
```
但这种普通虚拟网桥的方式，大量Docker容器时可能引起广播风暴，导致主机所在网络性能的下降。
VLAN技术将一个二层网络的机器隔离开来，在传统的以太网帧中添加一个VLAN tag字段，使用交换机的access端口和trunk端口。
Open vSwitch是一个开源的虚拟交换机软件，可以直接建立多种通信通道（如隧道[Open vSwitch with GRE/VxLan]）。
多主机VLAN情况下，首先要用桥接或直接路由两种方式实现跨主机通信，直接路由需要配置路由，很麻烦，但可以使用桥接，使用桥接就需要两块网卡（这段话不懂没关系，读完第一部分所有在回来读就明白了），一个网卡连接外网，一个网卡用于连接Open vSwitch网桥。
这里要求必须是两个网卡，eth0连接外网，eth1网卡连接docker0网桥，并且eth1需要开启混合模式。
1.在宿主机上用ovs创建一个非普通的支持VLAN功能的网桥ovs0，把网卡eth1添加到ovs0上。
2.创建一对veth设备对，一段添加到容器的网络命名空间中，重命名为eth0，并指定一个二层网络的IP，网络VLAN tag。一端添加到网桥ovs0上。
例子：
![enter description here][2]
所有容器内和外面是同一个子网192.10.x.x，我们使用VLAN做隔离，host1的con1和host2上的con3属于VLAN100，con2和con4属于VLAN200.由于会有VLAN ID为100和VLAN ID为200的帧通过，物理交换机上连接host1和host2的端口应设置为trunk端口。host1和host2上eth1没有设置VLAN的限制（trunk），是允许所有帧通过的。
具体完成步骤：
```shell?linenums
#在host1上
$ docker run -itd --name con1 ubuntu /bin/bash
$ docker run -itd --name con2 ubuntu /bin/bash
$ pipework ovs0 con1 192.168.0.1/24 @100
$ pipework ovs0 con1 192.168.0.2/24 @200
$ ovs-vsctl add-port ovs0 eth1;

#在host2上
$ docker run -itd --name con3 ubuntu /bin/bash
$ docker run -itd --name con4 ubuntu /bin/bash
$ pipework ovs0 con3 192.168.0.3/24 @100
$ pipework ovs0 con4 192.168.0.4/24 @200
#此端口则转发所有帧
$ ovs-vsctl add-port ovs0 eth1;
```
		乐视leengine的网络拓扑就是这种方式：
		为了进一步保证业务容器在网络方面的稳定性，所有的计算节点都是4个网卡，2千兆，2万兆，双双做bond，千兆bond1用来做管理网卡，万兆bond1用来跑业务容器，每个计算节点在交付时候会创建一个OVS网桥，并将bond1挂载上去，上联交换机做堆叠，计算节点尽量打散在多个不同机柜中。
		计算节点物理机上的Kubulet在创建Pod的PAUSE容器后，会调用我们自己CNI插件，CNI会创建一个veth pair， 一端扔到这个容器的Namespace中，并命名eth0，一端挂载到OVS网桥上，之后从etcd中大的IP段中找出一个连续16个IP地址的小段给这个计算节点，然后再从这个子段中找一个空闲的IP给这个容器，配置好容器IP，以及路由信息，同时会根据配置来确定是否发送免费ARP信息，之后遵守CNI规范把相应的信息返回给kubelet。当这个计算节点再次创建新的Pod时，会优先从这个子段中选择空间的IP，若没有空闲的IP地址，会重新计算一个子段给这个计算节点用。
		现在CNI不能保证Pod删掉重新创建时候IP保持不变，因此应用在每次升级操作后，容器IP地址会变，这就需要我们的服务发现与负载均衡做对接。
		不过现在的这套方案也会存在一些问题：比如物理主机突然down掉，或者Docker进程死掉，导致本主机上所有容器挂掉，等kubelet重新启动后，原来死掉的容器所占用的IP并不会释放。我们现在的解决方案是通过我们开发CNICTL命令来进行定期检测。CNICTL提供一个check命令，会检索etcd中所有分配的IP和对应的POD信息，之后调用apiserver获得所有Pod信息，取差值则为没释放的IP地址。收到报警后，人工调用CNICTL的释放IP功能，手动释放IP。


* 使用macvlan设备将容器连接到本地网络
除了使用Linux Bridge，上面普通linux 虚拟网桥或ovs网桥，将Docker容器桥接到本地网络之外，还有另外一种方式，即使用主机网卡的macvlan子设备。macvlan设备是从网卡上虚拟出一块新网卡，它和主网卡分别有不同的MAC地址，可以配置独立的IP地址。目前Docker网络本身不提供macvlan支持，但可以借助pipework来完成macvlan配置。如果采用macvlan来完成之前例子，那么整个过程只需要执行一条命令：

	* 不再一个大二层网络
* 宿主机间不再一个子网情况下


  [1]: ./images/1498791135386.jpg
  [2]: ./images/1498791262051.jpg